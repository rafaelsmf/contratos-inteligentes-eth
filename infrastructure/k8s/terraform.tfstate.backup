{
  "version": 4,
  "terraform_version": "1.9.8",
  "serial": 48,
  "lineage": "ed5133e5-dead-78c6-85dc-360d908c32f0",
  "outputs": {},
  "resources": [
    {
      "mode": "managed",
      "type": "helm_release",
      "name": "airflow",
      "provider": "provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "atomic": false,
            "chart": "airflow",
            "cleanup_on_fail": false,
            "create_namespace": false,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "airflow",
            "keyring": null,
            "lint": false,
            "manifest": null,
            "max_history": 0,
            "metadata": [
              {
                "app_version": "2.9.3",
                "chart": "airflow",
                "first_deployed": 1729259155,
                "last_deployed": 1729259155,
                "name": "airflow",
                "namespace": "airflow",
                "notes": "\n\n\nThank you for installing Apache Airflow 2.9.3!\n\nYour release is named airflow.\nYou can now access your dashboard(s) by executing the following command(s) and visiting the corresponding port at localhost in your browser:\n\nAirflow Webserver:     kubectl port-forward svc/airflow-webserver 8080:8080 --namespace airflow\nDefault Webserver (Airflow UI) Login credentials:\n    username: admin\n    password: admin\nDefault Postgres connection credentials:\n    username: postgres\n    password: postgres\n    port: 5432\n\nYou can get Fernet Key value by running the following:\n\n    echo Fernet Key: $(kubectl get secret --namespace airflow airflow-fernet-key -o jsonpath=\"{.data.fernet-key}\" | base64 --decode)\n\nWARNING:\n    Kubernetes workers task logs may not persist unless you configure log persistence or remote logging!\n    Logging options can be found at: https://airflow.apache.org/docs/helm-chart/stable/manage-logs.html\n    (This warning can be ignored if logging is configured with environment variables or secrets backend)\n\n###########################################################\n#  WARNING: You should set a static webserver secret key  #\n###########################################################\n\nYou are using a dynamically generated webserver secret key, which can lead to\nunnecessary restarts of your Airflow components.\n\nInformation on how to set a static webserver secret key can be found here:\nhttps://airflow.apache.org/docs/helm-chart/stable/production-guide.html#webserver-secret-key\n\nCHART NAME: postgresql\nCHART VERSION: 13.2.24\nAPP VERSION: 16.1.0\n\n** Please be patient while the chart is being deployed **\n\nPostgreSQL can be accessed via port 5432 on the following DNS names from within your cluster:\n\n    airflow-postgresql.airflow.svc.cluster.local - Read/Write connection\n\nTo get the password for \"postgres\" run:\n\n    export POSTGRES_PASSWORD=$(kubectl get secret --namespace airflow airflow-postgresql -o jsonpath=\"{.data.postgres-password}\" | base64 -d)\n\nTo connect to your database run the following command:\n\n    kubectl run airflow-postgresql-client --rm --tty -i --restart='Never' --namespace airflow --image docker.io/bitnami/postgresql:16.1.0-debian-11-r15 --env=\"PGPASSWORD=$POSTGRES_PASSWORD\" \\\n      --command -- psql --host airflow-postgresql -U postgres -d postgres -p 5432\n\n    \u003e NOTE: If you access the container using bash, make sure that you execute \"/opt/bitnami/scripts/postgresql/entrypoint.sh /bin/bash\" in order to avoid the error \"psql: local user with ID 1001} does not exist\"\n\nTo connect to your database from outside the cluster execute the following commands:\n\n    kubectl port-forward --namespace airflow svc/airflow-postgresql 5432:5432 \u0026\n    PGPASSWORD=\"$POSTGRES_PASSWORD\" psql --host 127.0.0.1 -U postgres -d postgres -p 5432\n\nWARNING: The configured password will be ignored on new installation in case when previous PostgreSQL release was deleted through the helm command. In that case, old PVC will have an old password, and setting it through helm won't take effect. Deleting persistent volumes (PVs) will solve the issue.\n",
                "revision": 1,
                "values": "{\"_rpcServer\":{\"args\":[\"-c\",\"exec airflow internal-api\"],\"command\":[\"bash\"],\"enabled\":false,\"env\":[],\"extraContainers\":[],\"extraNetworkPolicies\":[],\"labels\":{},\"livenessProbe\":{\"failureThreshold\":5,\"initialDelaySeconds\":15,\"periodSeconds\":10,\"scheme\":\"HTTP\",\"timeoutSeconds\":5},\"networkPolicy\":{\"ingress\":{\"from\":[],\"ports\":[{\"port\":\"{{ .Values.ports._rpcServer }}\"}]}},\"podDisruptionBudget\":{\"config\":{\"maxUnavailable\":1},\"enabled\":false},\"readinessProbe\":{\"failureThreshold\":5,\"initialDelaySeconds\":15,\"periodSeconds\":10,\"scheme\":\"HTTP\",\"timeoutSeconds\":5},\"resources\":{},\"securityContexts\":{\"container\":{},\"pod\":{}},\"service\":{\"annotations\":{},\"loadBalancerIP\":null,\"loadBalancerSourceRanges\":[],\"ports\":[{\"name\":\"rpc-server\",\"port\":\"{{ .Values.ports._rpcServer }}\"}],\"type\":\"ClusterIP\"},\"serviceAccount\":{\"annotations\":{},\"automountServiceAccountToken\":true,\"create\":true,\"name\":null},\"startupProbe\":{\"failureThreshold\":6,\"periodSeconds\":10,\"scheme\":\"HTTP\",\"timeoutSeconds\":20},\"waitForMigrations\":{\"enabled\":true,\"env\":[],\"securityContexts\":{\"container\":{}}}},\"affinity\":{},\"airflowConfigAnnotations\":{},\"airflowHome\":\"/opt/airflow\",\"airflowLocalSettings\":\"{{- if semverCompare \\\"\\u003e=2.2.0\\\" .Values.airflowVersion }}\\n{{- if not (or .Values.webserverSecretKey .Values.webserverSecretKeySecretName) }}\\nfrom airflow.www.utils import UIAlert\\n\\nDASHBOARD_UIALERTS = [\\n  UIAlert(\\n    'Usage of a dynamic webserver secret key detected. We recommend a static webserver secret key instead.'\\n    ' See the \\u003ca href='\\n    '\\\"https://airflow.apache.org/docs/helm-chart/stable/production-guide.html#webserver-secret-key\\\" '\\n    'target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\"\\u003e'\\n    'Helm Chart Production Guide\\u003c/a\\u003e for more details.',\\n    category=\\\"warning\\\",\\n    roles=[\\\"Admin\\\"],\\n    html=True,\\n  )\\n]\\n{{- end }}\\n{{- end }}\",\"airflowPodAnnotations\":{},\"airflowVersion\":\"2.9.3\",\"allowPodLaunching\":true,\"cleanup\":{\"affinity\":{},\"args\":[\"bash\",\"-c\",\"exec airflow kubernetes cleanup-pods --namespace={{ .Release.Namespace }}\"],\"command\":null,\"containerLifecycleHooks\":{},\"enabled\":false,\"env\":[],\"failedJobsHistoryLimit\":null,\"jobAnnotations\":{},\"labels\":{},\"nodeSelector\":{},\"podAnnotations\":{},\"priorityClassName\":null,\"resources\":{},\"schedule\":\"*/15 * * * *\",\"securityContext\":{},\"securityContexts\":{\"container\":{},\"pod\":{}},\"serviceAccount\":{\"annotations\":{},\"automountServiceAccountToken\":true,\"create\":true,\"name\":null},\"successfulJobsHistoryLimit\":null,\"tolerations\":[],\"topologySpreadConstraints\":[]},\"config\":{\"celery\":{\"flower_url_prefix\":\"{{ ternary \\\"\\\" .Values.ingress.flower.path (eq .Values.ingress.flower.path \\\"/\\\") }}\",\"worker_concurrency\":16},\"celery_kubernetes_executor\":{\"kubernetes_queue\":\"kubernetes\"},\"core\":{\"colored_console_log\":\"False\",\"dags_folder\":\"{{ include \\\"airflow_dags\\\" . }}\",\"executor\":\"{{ .Values.executor }}\",\"load_examples\":\"False\",\"remote_logging\":\"{{- ternary \\\"True\\\" \\\"False\\\" .Values.elasticsearch.enabled }}\"},\"elasticsearch\":{\"json_format\":\"True\",\"log_id_template\":\"{dag_id}_{task_id}_{execution_date}_{try_number}\"},\"elasticsearch_configs\":{\"max_retries\":3,\"retry_timeout\":\"True\",\"timeout\":30},\"kerberos\":{\"ccache\":\"{{ .Values.kerberos.ccacheMountPath }}/{{ .Values.kerberos.ccacheFileName }}\",\"keytab\":\"{{ .Values.kerberos.keytabPath }}\",\"principal\":\"{{ .Values.kerberos.principal }}\",\"reinit_frequency\":\"{{ .Values.kerberos.reinitFrequency }}\"},\"kubernetes\":{\"airflow_configmap\":\"{{ include \\\"airflow_config\\\" . }}\",\"airflow_local_settings_configmap\":\"{{ include \\\"airflow_config\\\" . }}\",\"multi_namespace_mode\":\"{{ ternary \\\"True\\\" \\\"False\\\" .Values.multiNamespaceMode }}\",\"namespace\":\"{{ .Release.Namespace }}\",\"pod_template_file\":\"{{ include \\\"airflow_pod_template_file\\\" . }}/pod_template_file.yaml\",\"worker_container_repository\":\"{{ .Values.images.airflow.repository | default .Values.defaultAirflowRepository }}\",\"worker_container_tag\":\"{{ .Values.images.airflow.tag | default .Values.defaultAirflowTag }}\"},\"kubernetes_executor\":{\"multi_namespace_mode\":\"{{ ternary \\\"True\\\" \\\"False\\\" .Values.multiNamespaceMode }}\",\"namespace\":\"{{ .Release.Namespace }}\",\"pod_template_file\":\"{{ include \\\"airflow_pod_template_file\\\" . }}/pod_template_file.yaml\",\"worker_container_repository\":\"{{ .Values.images.airflow.repository | default .Values.defaultAirflowRepository }}\",\"worker_container_tag\":\"{{ .Values.images.airflow.tag | default .Values.defaultAirflowTag }}\"},\"logging\":{\"colored_console_log\":\"False\",\"remote_logging\":\"{{- ternary \\\"True\\\" \\\"False\\\" .Values.elasticsearch.enabled }}\"},\"metrics\":{\"statsd_host\":\"{{ printf \\\"%s-statsd\\\" (include \\\"airflow.fullname\\\" .) }}\",\"statsd_on\":\"{{ ternary \\\"True\\\" \\\"False\\\" .Values.statsd.enabled }}\",\"statsd_port\":9125,\"statsd_prefix\":\"airflow\"},\"scheduler\":{\"run_duration\":41460,\"standalone_dag_processor\":\"{{ ternary \\\"True\\\" \\\"False\\\" .Values.dagProcessor.enabled }}\",\"statsd_host\":\"{{ printf \\\"%s-statsd\\\" (include \\\"airflow.fullname\\\" .) }}\",\"statsd_on\":\"{{ ternary \\\"True\\\" \\\"False\\\" .Values.statsd.enabled }}\",\"statsd_port\":9125,\"statsd_prefix\":\"airflow\"},\"triggerer\":{\"default_capacity\":1000},\"webserver\":{\"enable_proxy_fix\":\"True\",\"rbac\":\"True\"}},\"containerLifecycleHooks\":{},\"createUserJob\":{\"affinity\":{},\"annotations\":{},\"applyCustomEnv\":true,\"args\":[\"bash\",\"-c\",\"exec \\\\\\nairflow {{ semverCompare \\\"\\u003e=2.0.0\\\" .Values.airflowVersion | ternary \\\"users create\\\" \\\"create_user\\\" }} \\\"$@\\\"\",\"--\",\"-r\",\"{{ .Values.webserver.defaultUser.role }}\",\"-u\",\"{{ .Values.webserver.defaultUser.username }}\",\"-e\",\"{{ .Values.webserver.defaultUser.email }}\",\"-f\",\"{{ .Values.webserver.defaultUser.firstName }}\",\"-l\",\"{{ .Values.webserver.defaultUser.lastName }}\",\"-p\",\"{{ .Values.webserver.defaultUser.password }}\"],\"command\":null,\"containerLifecycleHooks\":{},\"env\":[],\"extraContainers\":[],\"extraInitContainers\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"jobAnnotations\":{},\"labels\":{},\"nodeSelector\":{},\"priorityClassName\":null,\"resources\":{},\"securityContext\":{},\"securityContexts\":{\"container\":{},\"pod\":{}},\"serviceAccount\":{\"annotations\":{},\"automountServiceAccountToken\":true,\"create\":true,\"name\":null},\"tolerations\":[],\"topologySpreadConstraints\":[],\"ttlSecondsAfterFinished\":300,\"useHelmHooks\":true},\"dagProcessor\":{\"affinity\":{},\"annotations\":{},\"args\":[\"bash\",\"-c\",\"exec airflow dag-processor\"],\"command\":null,\"containerLifecycleHooks\":{},\"enabled\":false,\"env\":[],\"extraContainers\":[],\"extraInitContainers\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"livenessProbe\":{\"command\":null,\"failureThreshold\":5,\"initialDelaySeconds\":10,\"periodSeconds\":60,\"timeoutSeconds\":20},\"logGroomerSidecar\":{\"args\":[\"bash\",\"/clean-logs\"],\"command\":null,\"enabled\":true,\"resources\":{},\"retentionDays\":15,\"securityContexts\":{\"container\":{}}},\"nodeSelector\":{},\"podAnnotations\":{},\"priorityClassName\":null,\"replicas\":1,\"resources\":{},\"revisionHistoryLimit\":null,\"safeToEvict\":true,\"securityContext\":{},\"securityContexts\":{\"container\":{},\"pod\":{}},\"serviceAccount\":{\"annotations\":{},\"automountServiceAccountToken\":true,\"create\":true,\"name\":null},\"strategy\":{\"rollingUpdate\":{\"maxSurge\":\"100%\",\"maxUnavailable\":\"50%\"}},\"terminationGracePeriodSeconds\":60,\"tolerations\":[],\"topologySpreadConstraints\":[],\"waitForMigrations\":{\"enabled\":true,\"env\":[],\"securityContexts\":{\"container\":{}}}},\"dags\":{\"gitSync\":{\"branch\":\"develop\",\"containerLifecycleHooks\":{},\"containerName\":\"git-sync\",\"credentialsSecret\":\"git-credentials\",\"depth\":1,\"enabled\":true,\"env\":[],\"envFrom\":null,\"extraVolumeMounts\":[],\"maxFailures\":0,\"period\":\"5s\",\"ref\":\"develop\",\"repo\":\"https://github.com/rafaelsmf/contratos-inteligentes-eth.git\",\"resources\":{},\"rev\":\"HEAD\",\"securityContext\":{},\"securityContexts\":{\"container\":{}},\"subPath\":\"applications/airflow/dags\",\"uid\":65533,\"wait\":null},\"mountPath\":null,\"persistence\":{\"accessMode\":\"ReadWriteOnce\",\"annotations\":{},\"enabled\":false,\"existingClaim\":null,\"size\":\"1Gi\",\"storageClassName\":null,\"subPath\":null}},\"data\":{\"brokerUrl\":null,\"brokerUrlSecretName\":null,\"metadataConnection\":{\"db\":\"postgres\",\"host\":null,\"pass\":\"postgres\",\"port\":5432,\"protocol\":\"postgresql\",\"sslmode\":\"disable\",\"user\":\"postgres\"},\"metadataSecretName\":null,\"resultBackendConnection\":null,\"resultBackendSecretName\":null},\"defaultAirflowDigest\":null,\"defaultAirflowRepository\":\"apache/airflow\",\"defaultAirflowTag\":\"2.9.3\",\"elasticsearch\":{\"connection\":{},\"enabled\":false,\"secretName\":null},\"enableBuiltInSecretEnvVars\":{\"AIRFLOW_CONN_AIRFLOW_DB\":true,\"AIRFLOW__CELERY__BROKER_URL\":true,\"AIRFLOW__CELERY__CELERY_RESULT_BACKEND\":true,\"AIRFLOW__CELERY__RESULT_BACKEND\":true,\"AIRFLOW__CORE__FERNET_KEY\":true,\"AIRFLOW__CORE__SQL_ALCHEMY_CONN\":true,\"AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\":true,\"AIRFLOW__ELASTICSEARCH__ELASTICSEARCH_HOST\":true,\"AIRFLOW__ELASTICSEARCH__HOST\":true,\"AIRFLOW__WEBSERVER__SECRET_KEY\":true},\"env\":[],\"executor\":\"KubernetesExecutor\",\"extraConfigMaps\":{},\"extraEnv\":null,\"extraEnvFrom\":null,\"extraSecrets\":{},\"fernetKey\":null,\"fernetKeySecretName\":null,\"flower\":{\"affinity\":{},\"annotations\":{},\"args\":[\"bash\",\"-c\",\"exec \\\\\\nairflow {{ semverCompare \\\"\\u003e=2.0.0\\\" .Values.airflowVersion | ternary \\\"celery flower\\\" \\\"flower\\\" }}\"],\"command\":null,\"containerLifecycleHooks\":{},\"enabled\":false,\"env\":[],\"extraContainers\":[],\"extraNetworkPolicies\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"labels\":{},\"livenessProbe\":{\"failureThreshold\":10,\"initialDelaySeconds\":10,\"periodSeconds\":5,\"timeoutSeconds\":5},\"networkPolicy\":{\"ingress\":{\"from\":[],\"ports\":[{\"port\":\"{{ .Values.ports.flowerUI }}\"}]}},\"nodeSelector\":{},\"password\":null,\"podAnnotations\":{},\"priorityClassName\":null,\"readinessProbe\":{\"failureThreshold\":10,\"initialDelaySeconds\":10,\"periodSeconds\":5,\"timeoutSeconds\":5},\"resources\":{},\"revisionHistoryLimit\":null,\"secretName\":null,\"securityContext\":{},\"securityContexts\":{\"container\":{},\"pod\":{}},\"service\":{\"annotations\":{},\"loadBalancerIP\":null,\"loadBalancerSourceRanges\":[],\"ports\":[{\"name\":\"flower-ui\",\"port\":\"{{ .Values.ports.flowerUI }}\"}],\"type\":\"ClusterIP\"},\"serviceAccount\":{\"annotations\":{},\"automountServiceAccountToken\":true,\"create\":true,\"name\":null},\"tolerations\":[],\"topologySpreadConstraints\":[],\"username\":null},\"fullnameOverride\":\"\",\"gid\":0,\"images\":{\"airflow\":{\"digest\":null,\"pullPolicy\":\"IfNotPresent\",\"repository\":null,\"tag\":null},\"flower\":{\"pullPolicy\":\"IfNotPresent\",\"repository\":null,\"tag\":null},\"gitSync\":{\"pullPolicy\":\"IfNotPresent\",\"repository\":\"registry.k8s.io/git-sync/git-sync\",\"tag\":\"v4.1.0\"},\"migrationsWaitTimeout\":60,\"pgbouncer\":{\"pullPolicy\":\"IfNotPresent\",\"repository\":\"apache/airflow\",\"tag\":\"airflow-pgbouncer-2024.01.19-1.21.0\"},\"pgbouncerExporter\":{\"pullPolicy\":\"IfNotPresent\",\"repository\":\"apache/airflow\",\"tag\":\"airflow-pgbouncer-exporter-2024.06.18-0.17.0\"},\"pod_template\":{\"pullPolicy\":\"IfNotPresent\",\"repository\":null,\"tag\":null},\"redis\":{\"pullPolicy\":\"IfNotPresent\",\"repository\":\"redis\",\"tag\":\"7.2-bookworm\"},\"statsd\":{\"pullPolicy\":\"IfNotPresent\",\"repository\":\"quay.io/prometheus/statsd-exporter\",\"tag\":\"v0.26.1\"},\"useDefaultImageForMigration\":false},\"ingress\":{\"enabled\":null,\"flower\":{\"annotations\":{},\"enabled\":false,\"host\":\"\",\"hosts\":[],\"ingressClassName\":\"\",\"path\":\"/\",\"pathType\":\"ImplementationSpecific\",\"tls\":{\"enabled\":false,\"secretName\":\"\"}},\"web\":{\"annotations\":{},\"enabled\":false,\"host\":\"\",\"hosts\":[],\"ingressClassName\":\"\",\"path\":\"/\",\"pathType\":\"ImplementationSpecific\",\"precedingPaths\":[],\"succeedingPaths\":[],\"tls\":{\"enabled\":false,\"secretName\":\"\"}}},\"kerberos\":{\"ccacheFileName\":\"cache\",\"ccacheMountPath\":\"/var/kerberos-ccache\",\"config\":\"# This is an example config showing how you can use templating and how \\\"example\\\" config\\n# might look like. It works with the test kerberos server that we are using during integration\\n# testing at Apache Airflow (see `scripts/ci/docker-compose/integration-kerberos.yml` but in\\n# order to make it production-ready you must replace it with your own configuration that\\n# Matches your kerberos deployment. Administrators of your Kerberos instance should\\n# provide the right configuration.\\n\\n[logging]\\ndefault = \\\"FILE:{{ template \\\"airflow_logs_no_quote\\\" . }}/kerberos_libs.log\\\"\\nkdc = \\\"FILE:{{ template \\\"airflow_logs_no_quote\\\" . }}/kerberos_kdc.log\\\"\\nadmin_server = \\\"FILE:{{ template \\\"airflow_logs_no_quote\\\" . }}/kadmind.log\\\"\\n\\n[libdefaults]\\ndefault_realm = FOO.COM\\nticket_lifetime = 10h\\nrenew_lifetime = 7d\\nforwardable = true\\n\\n[realms]\\nFOO.COM = {\\n  kdc = kdc-server.foo.com\\n  admin_server = admin_server.foo.com\\n}\\n\",\"configPath\":\"/etc/krb5.conf\",\"enabled\":false,\"keytabBase64Content\":null,\"keytabPath\":\"/etc/airflow.keytab\",\"principal\":\"airflow@FOO.COM\",\"reinitFrequency\":3600},\"labels\":{},\"limits\":[],\"logs\":{\"persistence\":{\"annotations\":{},\"enabled\":false,\"existingClaim\":null,\"size\":\"1Gi\",\"storageClassName\":null}},\"migrateDatabaseJob\":{\"affinity\":{},\"annotations\":{},\"applyCustomEnv\":true,\"args\":[\"bash\",\"-c\",\"exec \\\\\\nairflow {{ semverCompare \\\"\\u003e=2.7.0\\\" .Values.airflowVersion | ternary \\\"db migrate\\\" (semverCompare \\\"\\u003e=2.0.0\\\" .Values.airflowVersion | ternary \\\"db upgrade\\\" \\\"upgradedb\\\") }}\"],\"command\":null,\"containerLifecycleHooks\":{},\"enabled\":true,\"extraContainers\":[],\"extraInitContainers\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"jobAnnotations\":{},\"labels\":{},\"nodeSelector\":{},\"priorityClassName\":null,\"resources\":{},\"securityContext\":{},\"securityContexts\":{\"container\":{},\"pod\":{}},\"serviceAccount\":{\"annotations\":{},\"automountServiceAccountToken\":true,\"create\":true,\"name\":null},\"tolerations\":[],\"topologySpreadConstraints\":[],\"ttlSecondsAfterFinished\":300,\"useHelmHooks\":true},\"multiNamespaceMode\":false,\"nameOverride\":\"\",\"networkPolicies\":{\"enabled\":false},\"nodeSelector\":{},\"pgbouncer\":{\"affinity\":{},\"annotations\":{},\"args\":null,\"auth_file\":\"/etc/pgbouncer/users.txt\",\"auth_type\":\"scram-sha-256\",\"ciphers\":\"normal\",\"command\":[\"pgbouncer\",\"-u\",\"nobody\",\"/etc/pgbouncer/pgbouncer.ini\"],\"configSecretName\":null,\"containerLifecycleHooks\":{\"preStop\":{\"exec\":{\"command\":[\"/bin/sh\",\"-c\",\"killall -INT pgbouncer \\u0026\\u0026 sleep 120\"]}}},\"enabled\":false,\"env\":[],\"extraContainers\":[],\"extraIni\":null,\"extraIniMetadata\":null,\"extraIniResultBackend\":null,\"extraNetworkPolicies\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"logConnections\":0,\"logDisconnections\":0,\"maxClientConn\":100,\"metadataPoolSize\":10,\"metricsExporterSidecar\":{\"containerLifecycleHooks\":{},\"livenessProbe\":{\"initialDelaySeconds\":10,\"periodSeconds\":10,\"timeoutSeconds\":1},\"readinessProbe\":{\"initialDelaySeconds\":10,\"periodSeconds\":10,\"timeoutSeconds\":1},\"resources\":{},\"securityContexts\":{\"container\":{}},\"sslmode\":\"disable\",\"statsSecretKey\":null,\"statsSecretName\":null},\"nodeSelector\":{},\"podAnnotations\":{},\"podDisruptionBudget\":{\"config\":{\"maxUnavailable\":1},\"enabled\":false},\"priorityClassName\":null,\"replicas\":1,\"resources\":{},\"resultBackendPoolSize\":5,\"revisionHistoryLimit\":null,\"securityContexts\":{\"container\":{},\"pod\":{}},\"service\":{\"extraAnnotations\":{}},\"serviceAccount\":{\"annotations\":{},\"automountServiceAccountToken\":true,\"create\":true,\"name\":null},\"ssl\":{\"ca\":null,\"cert\":null,\"key\":null},\"sslmode\":\"prefer\",\"tolerations\":[],\"topologySpreadConstraints\":[],\"uid\":65534,\"verbose\":0},\"podTemplate\":null,\"ports\":{\"_rpcServer\":9080,\"airflowUI\":8080,\"flowerUI\":5555,\"pgbouncer\":6543,\"pgbouncerScrape\":9127,\"redisDB\":6379,\"statsdIngest\":9125,\"statsdScrape\":9102,\"triggererLogs\":8794,\"workerLogs\":8793},\"postgresql\":{\"auth\":{\"enablePostgresUser\":true,\"password\":\"\",\"postgresPassword\":\"postgres\",\"username\":\"\"},\"enabled\":true},\"priorityClasses\":[],\"quotas\":{},\"rbac\":{\"create\":true,\"createSCCRoleBinding\":false},\"redis\":{\"affinity\":{},\"annotations\":{},\"containerLifecycleHooks\":{},\"enabled\":true,\"nodeSelector\":{},\"password\":null,\"passwordSecretName\":null,\"persistence\":{\"annotations\":{},\"enabled\":true,\"size\":\"1Gi\",\"storageClassName\":null},\"podAnnotations\":{},\"priorityClassName\":null,\"resources\":{},\"safeToEvict\":true,\"securityContext\":{},\"securityContexts\":{\"container\":{},\"pod\":{}},\"serviceAccount\":{\"annotations\":{},\"automountServiceAccountToken\":true,\"create\":true,\"name\":null},\"terminationGracePeriodSeconds\":600,\"tolerations\":[],\"topologySpreadConstraints\":[],\"uid\":0},\"registry\":{\"connection\":{},\"secretName\":null},\"revisionHistoryLimit\":null,\"scheduler\":{\"affinity\":{},\"annotations\":{},\"args\":[\"bash\",\"-c\",\"exec airflow scheduler\"],\"command\":null,\"containerLifecycleHooks\":{},\"enabled\":true,\"env\":[],\"extraContainers\":[],\"extraInitContainers\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"hostAliases\":[],\"labels\":{},\"livenessProbe\":{\"command\":null,\"failureThreshold\":5,\"initialDelaySeconds\":10,\"periodSeconds\":60,\"timeoutSeconds\":20},\"logGroomerSidecar\":{\"args\":[\"bash\",\"/clean-logs\"],\"command\":null,\"containerLifecycleHooks\":{},\"enabled\":true,\"resources\":{},\"retentionDays\":15,\"securityContexts\":{\"container\":{}}},\"nodeSelector\":{},\"podAnnotations\":{},\"podDisruptionBudget\":{\"config\":{\"maxUnavailable\":1},\"enabled\":false},\"priorityClassName\":null,\"replicas\":1,\"resources\":{},\"revisionHistoryLimit\":null,\"safeToEvict\":true,\"securityContext\":{},\"securityContexts\":{\"container\":{},\"pod\":{}},\"serviceAccount\":{\"annotations\":{},\"automountServiceAccountToken\":true,\"create\":true,\"name\":null},\"startupProbe\":{\"command\":null,\"failureThreshold\":6,\"periodSeconds\":10,\"timeoutSeconds\":20},\"strategy\":null,\"tolerations\":[],\"topologySpreadConstraints\":[],\"updateStrategy\":null,\"waitForMigrations\":{\"enabled\":true,\"env\":[],\"securityContexts\":{\"container\":{}}}},\"schedulerName\":null,\"secret\":[],\"securityContext\":{},\"securityContexts\":{\"containers\":{},\"pod\":{}},\"statsd\":{\"affinity\":{},\"annotations\":{},\"args\":[\"--statsd.mapping-config=/etc/statsd-exporter/mappings.yml\"],\"configMapAnnotations\":{},\"containerLifecycleHooks\":{},\"enabled\":true,\"env\":[],\"extraMappings\":[],\"extraNetworkPolicies\":[],\"nodeSelector\":{},\"overrideMappings\":[],\"podAnnotations\":{},\"priorityClassName\":null,\"resources\":{},\"revisionHistoryLimit\":null,\"securityContext\":{},\"securityContexts\":{\"container\":{},\"pod\":{}},\"service\":{\"extraAnnotations\":{}},\"serviceAccount\":{\"annotations\":{},\"automountServiceAccountToken\":true,\"create\":true,\"name\":null},\"tolerations\":[],\"topologySpreadConstraints\":[],\"uid\":65534},\"tolerations\":[],\"topologySpreadConstraints\":[],\"triggerer\":{\"affinity\":{},\"annotations\":{},\"args\":[\"bash\",\"-c\",\"exec airflow triggerer\"],\"command\":null,\"containerLifecycleHooks\":{},\"enabled\":true,\"env\":[],\"extraContainers\":[],\"extraInitContainers\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"keda\":{\"advanced\":{},\"cooldownPeriod\":30,\"enabled\":false,\"maxReplicaCount\":10,\"minReplicaCount\":0,\"namespaceLabels\":{},\"pollingInterval\":5,\"query\":\"SELECT ceil(COUNT(*)::decimal / {{ .Values.config.triggerer.default_capacity }}) FROM trigger\",\"usePgbouncer\":false},\"labels\":{},\"livenessProbe\":{\"command\":null,\"failureThreshold\":5,\"initialDelaySeconds\":10,\"periodSeconds\":60,\"timeoutSeconds\":20},\"logGroomerSidecar\":{\"args\":[\"bash\",\"/clean-logs\"],\"command\":null,\"containerLifecycleHooks\":{},\"enabled\":true,\"resources\":{},\"retentionDays\":15,\"securityContexts\":{\"container\":{}}},\"nodeSelector\":{},\"persistence\":{\"annotations\":{},\"enabled\":true,\"fixPermissions\":false,\"persistentVolumeClaimRetentionPolicy\":null,\"size\":\"1Gi\",\"storageClassName\":null},\"podAnnotations\":{},\"priorityClassName\":null,\"replicas\":1,\"resources\":{},\"revisionHistoryLimit\":null,\"safeToEvict\":true,\"securityContext\":{},\"securityContexts\":{\"container\":{},\"pod\":{}},\"serviceAccount\":{\"annotations\":{},\"automountServiceAccountToken\":true,\"create\":true,\"name\":null},\"strategy\":{\"rollingUpdate\":{\"maxSurge\":\"100%\",\"maxUnavailable\":\"50%\"}},\"terminationGracePeriodSeconds\":60,\"tolerations\":[],\"topologySpreadConstraints\":[],\"updateStrategy\":null,\"waitForMigrations\":{\"enabled\":true,\"env\":[],\"securityContexts\":{\"container\":{}}}},\"uid\":50000,\"useStandardNaming\":false,\"volumeMounts\":[],\"volumes\":[],\"webserver\":{\"affinity\":{},\"allowPodLogReading\":true,\"annotations\":{},\"args\":[\"bash\",\"-c\",\"exec airflow webserver\"],\"command\":null,\"configMapAnnotations\":{},\"containerLifecycleHooks\":{},\"defaultUser\":{\"email\":\"admin@example.com\",\"enabled\":true,\"firstName\":\"admin\",\"lastName\":\"user\",\"password\":\"admin\",\"role\":\"Admin\",\"username\":\"admin\"},\"enabled\":true,\"env\":[],\"extraContainers\":[],\"extraInitContainers\":[],\"extraNetworkPolicies\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"hostAliases\":[],\"labels\":{},\"livenessProbe\":{\"failureThreshold\":5,\"initialDelaySeconds\":15,\"periodSeconds\":10,\"scheme\":\"HTTP\",\"timeoutSeconds\":5},\"networkPolicy\":{\"ingress\":{\"from\":[],\"ports\":[{\"port\":\"{{ .Values.ports.airflowUI }}\"}]}},\"nodeSelector\":{},\"podAnnotations\":{},\"podDisruptionBudget\":{\"config\":{\"maxUnavailable\":1},\"enabled\":false},\"priorityClassName\":null,\"readinessProbe\":{\"failureThreshold\":5,\"initialDelaySeconds\":15,\"periodSeconds\":10,\"scheme\":\"HTTP\",\"timeoutSeconds\":5},\"replicas\":1,\"resources\":{},\"revisionHistoryLimit\":null,\"securityContext\":{},\"securityContexts\":{\"container\":{},\"pod\":{}},\"service\":{\"annotations\":{},\"loadBalancerIP\":null,\"loadBalancerSourceRanges\":[],\"ports\":[{\"name\":\"airflow-ui\",\"port\":\"{{ .Values.ports.airflowUI }}\"}],\"type\":\"ClusterIP\"},\"serviceAccount\":{\"annotations\":{},\"automountServiceAccountToken\":true,\"create\":true,\"name\":null},\"startupProbe\":{\"failureThreshold\":6,\"periodSeconds\":10,\"scheme\":\"HTTP\",\"timeoutSeconds\":20},\"strategy\":null,\"tolerations\":[],\"topologySpreadConstraints\":[],\"waitForMigrations\":{\"enabled\":true,\"env\":[],\"securityContexts\":{\"container\":{}}},\"webserverConfig\":null,\"webserverConfigConfigMapName\":null},\"webserverSecretKey\":null,\"webserverSecretKeySecretName\":null,\"workers\":{\"affinity\":{},\"annotations\":{},\"args\":[\"bash\",\"-c\",\"exec \\\\\\nairflow {{ semverCompare \\\"\\u003e=2.0.0\\\" .Values.airflowVersion | ternary \\\"celery worker\\\" \\\"worker\\\" }}\"],\"command\":null,\"containerLifecycleHooks\":{},\"env\":[],\"extraContainers\":[],\"extraInitContainers\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"hostAliases\":[],\"hpa\":{\"behavior\":{},\"enabled\":false,\"maxReplicaCount\":5,\"metrics\":[{\"resource\":{\"name\":\"cpu\",\"target\":{\"averageUtilization\":80,\"type\":\"Utilization\"}},\"type\":\"Resource\"}],\"minReplicaCount\":0},\"keda\":{\"advanced\":{},\"cooldownPeriod\":30,\"enabled\":false,\"maxReplicaCount\":10,\"minReplicaCount\":0,\"namespaceLabels\":{},\"pollingInterval\":5,\"query\":\"SELECT ceil(COUNT(*)::decimal / {{ .Values.config.celery.worker_concurrency }}) FROM task_instance WHERE (state='running' OR state='queued') {{- if eq .Values.executor \\\"CeleryKubernetesExecutor\\\" }} AND queue != '{{ .Values.config.celery_kubernetes_executor.kubernetes_queue }}' {{- end }}\",\"usePgbouncer\":true},\"kerberosInitContainer\":{\"enabled\":false,\"resources\":{}},\"kerberosSidecar\":{\"containerLifecycleHooks\":{},\"enabled\":false,\"resources\":{},\"securityContexts\":{\"container\":{}}},\"labels\":{},\"livenessProbe\":{\"command\":null,\"enabled\":true,\"failureThreshold\":5,\"initialDelaySeconds\":10,\"periodSeconds\":60,\"timeoutSeconds\":20},\"logGroomerSidecar\":{\"args\":[\"bash\",\"/clean-logs\"],\"command\":null,\"enabled\":true,\"resources\":{},\"retentionDays\":15,\"securityContexts\":{\"container\":{}}},\"nodeSelector\":{},\"persistence\":{\"annotations\":{},\"containerLifecycleHooks\":{},\"enabled\":true,\"fixPermissions\":false,\"persistentVolumeClaimRetentionPolicy\":null,\"securityContexts\":{\"container\":{}},\"size\":\"1Gi\",\"storageClassName\":null},\"podAnnotations\":{},\"priorityClassName\":null,\"replicas\":1,\"resources\":{},\"revisionHistoryLimit\":null,\"runtimeClassName\":null,\"safeToEvict\":false,\"securityContext\":{},\"securityContexts\":{\"container\":{},\"pod\":{}},\"serviceAccount\":{\"annotations\":{},\"automountServiceAccountToken\":true,\"create\":true,\"name\":null},\"strategy\":{\"rollingUpdate\":{\"maxSurge\":\"100%\",\"maxUnavailable\":\"50%\"}},\"terminationGracePeriodSeconds\":600,\"tolerations\":[],\"topologySpreadConstraints\":[],\"updateStrategy\":null,\"volumeClaimTemplates\":[],\"waitForMigrations\":{\"enabled\":true,\"env\":[],\"securityContexts\":{\"container\":{}}}}}",
                "version": "1.15.0"
              }
            ],
            "name": "airflow",
            "namespace": "airflow",
            "pass_credentials": false,
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": "https://airflow.apache.org",
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [],
            "set_list": [],
            "set_sensitive": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 300,
            "upgrade_install": null,
            "values": [
              "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n---\n# Default values for airflow.\n# This is a YAML-formatted file.\n# Declare variables to be passed into your templates.\n\n# Provide a name to substitute for the full names of resources\nfullnameOverride: \"\"\n\n# Provide a name to substitute for the name of the chart\nnameOverride: \"\"\n\n# Use standard naming for all resources using airflow.fullname template\n# Consider removing this later and default it to true\n# to make this chart follow standard naming conventions using the fullname template.\n# For now this is an opt-in switch for backwards compatibility to leverage the standard naming convention\n# and being able to use fully fullnameOverride and nameOverride in all resources\n# For new installations - it is recommended to set it to True to follow standard naming conventions\n# For existing installations, this will rename and redeploy your resources with the new names. Be aware that\n# this will recreate your deployment/statefulsets along with their persistent volume claims and data storage\n# migration may be needed to keep your old data\n#\n# Note:fernet-key,redis-password and broker-url secrets don't use this logic yet,\n# as this may break existing installations due to how they get installed via pre-install hook.\nuseStandardNaming: false\n\n# Max number of old replicasets to retain. Can be overridden by each deployment's revisionHistoryLimit\nrevisionHistoryLimit: ~\n\n# User and group of airflow user\nuid: 50000\ngid: 0\n\n# Default security context for airflow (deprecated, use `securityContexts` instead)\nsecurityContext: {}\n#  runAsUser: 50000\n#  fsGroup: 0\n#  runAsGroup: 0\n\n# Detailed default security context for airflow deployments\nsecurityContexts:\n  pod: {}\n  containers: {}\n\n# Global container lifecycle hooks for airflow containers\ncontainerLifecycleHooks: {}\n\n# Airflow home directory\n# Used for mount paths\nairflowHome: /opt/airflow\n\n# Default airflow repository -- overridden by all the specific images below\ndefaultAirflowRepository: apache/airflow\n\n# Default airflow tag to deploy\ndefaultAirflowTag: \"2.9.3\"\n\n# Default airflow digest. If specified, it takes precedence over tag\ndefaultAirflowDigest: ~\n\n# Airflow version (Used to make some decisions based on Airflow Version being deployed)\nairflowVersion: \"2.9.3\"\n\n# Images\nimages:\n  airflow:\n    repository: ~\n    tag: ~\n    # Specifying digest takes precedence over tag.\n    digest: ~\n    pullPolicy: IfNotPresent\n  # To avoid images with user code, you can turn this to 'true' and\n  # all the 'run-airflow-migrations' and 'wait-for-airflow-migrations' containers/jobs\n  # will use the images from 'defaultAirflowRepository:defaultAirflowTag' values\n  # to run and wait for DB migrations .\n  useDefaultImageForMigration: false\n  # timeout (in seconds) for airflow-migrations to complete\n  migrationsWaitTimeout: 60\n  pod_template:\n    # Note that `images.pod_template.repository` and `images.pod_template.tag` parameters\n    # can be overridden in `config.kubernetes` section. So for these parameters to have effect\n    # `config.kubernetes.worker_container_repository` and `config.kubernetes.worker_container_tag`\n    # must be not set .\n    repository: ~\n    tag: ~\n    pullPolicy: IfNotPresent\n  flower:\n    repository: ~\n    tag: ~\n    pullPolicy: IfNotPresent\n  statsd:\n    repository: quay.io/prometheus/statsd-exporter\n    tag: v0.26.1\n    pullPolicy: IfNotPresent\n  redis:\n    repository: redis\n    # Redis is limited to 7.2-bookworm due to licencing change\n    # https://redis.io/blog/redis-adopts-dual-source-available-licensing/\n    tag: 7.2-bookworm\n    pullPolicy: IfNotPresent\n  pgbouncer:\n    repository: apache/airflow\n    tag: airflow-pgbouncer-2024.01.19-1.21.0\n    pullPolicy: IfNotPresent\n  pgbouncerExporter:\n    repository: apache/airflow\n    tag: airflow-pgbouncer-exporter-2024.06.18-0.17.0\n    pullPolicy: IfNotPresent\n  gitSync:\n    repository: registry.k8s.io/git-sync/git-sync\n    tag: v4.1.0\n    pullPolicy: IfNotPresent\n\n# Select certain nodes for airflow pods.\nnodeSelector: {}\naffinity: {}\ntolerations: []\ntopologySpreadConstraints: []\nschedulerName: ~\n\n# Add common labels to all objects and pods defined in this chart.\nlabels: {}\n\n# Ingress configuration\ningress:\n  # Enable all ingress resources (deprecated - use ingress.web.enabled and ingress.flower.enabled)\n  enabled: ~\n\n  # Configs for the Ingress of the web Service\n  web:\n    # Enable web ingress resource\n    enabled: false\n\n    # Annotations for the web Ingress\n    annotations: {}\n\n    # The path for the web Ingress\n    path: \"/\"\n\n    # The pathType for the above path (used only with Kubernetes v1.19 and above)\n    pathType: \"ImplementationSpecific\"\n\n    # The hostname for the web Ingress (Deprecated - renamed to `ingress.web.hosts`)\n    host: \"\"\n\n    # The hostnames or hosts configuration for the web Ingress\n    hosts: []\n    #   # The hostname for the web Ingress (can be templated)\n    # - name: \"\"\n    #   # configs for web Ingress TLS\n    #   tls:\n    #     # Enable TLS termination for the web Ingress\n    #     enabled: false\n    #     # the name of a pre-created Secret containing a TLS private key and certificate\n    #     secretName: \"\"\n\n    # The Ingress Class for the web Ingress (used only with Kubernetes v1.19 and above)\n    ingressClassName: \"\"\n\n    # configs for web Ingress TLS (Deprecated - renamed to `ingress.web.hosts[*].tls`)\n    tls:\n      # Enable TLS termination for the web Ingress\n      enabled: false\n      # the name of a pre-created Secret containing a TLS private key and certificate\n      secretName: \"\"\n\n    # HTTP paths to add to the web Ingress before the default path\n    precedingPaths: []\n\n    # Http paths to add to the web Ingress after the default path\n    succeedingPaths: []\n\n  # Configs for the Ingress of the flower Service\n  flower:\n    # Enable web ingress resource\n    enabled: false\n\n    # Annotations for the flower Ingress\n    annotations: {}\n\n    # The path for the flower Ingress\n    path: \"/\"\n\n    # The pathType for the above path (used only with Kubernetes v1.19 and above)\n    pathType: \"ImplementationSpecific\"\n\n    # The hostname for the flower Ingress (Deprecated - renamed to `ingress.flower.hosts`)\n    host: \"\"\n\n    # The hostnames or hosts configuration for the flower Ingress\n    hosts: []\n    #   # The hostname for the flower Ingress (can be templated)\n    # - name: \"\"\n    #   tls:\n    #     # Enable TLS termination for the flower Ingress\n    #     enabled: false\n    #     # the name of a pre-created Secret containing a TLS private key and certificate\n    #     secretName: \"\"\n\n    # The Ingress Class for the flower Ingress (used only with Kubernetes v1.19 and above)\n    ingressClassName: \"\"\n\n    # configs for flower Ingress TLS (Deprecated - renamed to `ingress.flower.hosts[*].tls`)\n    tls:\n      # Enable TLS termination for the flower Ingress\n      enabled: false\n      # the name of a pre-created Secret containing a TLS private key and certificate\n      secretName: \"\"\n\n# Network policy configuration\nnetworkPolicies:\n  # Enabled network policies\n  enabled: false\n\n# Extra annotations to apply to all\n# Airflow pods\nairflowPodAnnotations: {}\n\n# Extra annotations to apply to\n# main Airflow configmap\nairflowConfigAnnotations: {}\n\n# `airflow_local_settings` file as a string (can be templated).\nairflowLocalSettings: |-\n  {{- if semverCompare \"\u003e=2.2.0\" .Values.airflowVersion }}\n  {{- if not (or .Values.webserverSecretKey .Values.webserverSecretKeySecretName) }}\n  from airflow.www.utils import UIAlert\n\n  DASHBOARD_UIALERTS = [\n    UIAlert(\n      'Usage of a dynamic webserver secret key detected. We recommend a static webserver secret key instead.'\n      ' See the \u003ca href='\n      '\"https://airflow.apache.org/docs/helm-chart/stable/production-guide.html#webserver-secret-key\" '\n      'target=\"_blank\" rel=\"noopener noreferrer\"\u003e'\n      'Helm Chart Production Guide\u003c/a\u003e for more details.',\n      category=\"warning\",\n      roles=[\"Admin\"],\n      html=True,\n    )\n  ]\n  {{- end }}\n  {{- end }}\n\n# Enable RBAC (default on most clusters these days)\nrbac:\n  # Specifies whether RBAC resources should be created\n  create: true\n  createSCCRoleBinding: false\n\n# Airflow executor\n# One of: LocalExecutor, LocalKubernetesExecutor, CeleryExecutor, KubernetesExecutor, CeleryKubernetesExecutor\nexecutor: \"KubernetesExecutor\"\n\n# If this is true and using LocalExecutor/KubernetesExecutor/CeleryKubernetesExecutor, the scheduler's\n# service account will have access to communicate with the api-server and launch pods.\n# If this is true and using CeleryExecutor/KubernetesExecutor/CeleryKubernetesExecutor, the workers\n# will be able to launch pods.\nallowPodLaunching: true\n\n# Environment variables for all airflow containers\nenv: []\n# - name: \"\"\n#   value: \"\"\n\n# Volumes for all airflow containers\nvolumes: []\n\n# VolumeMounts for all airflow containers\nvolumeMounts: []\n\n# Secrets for all airflow containers\nsecret: []\n# - envName: \"\"\n#   secretName: \"\"\n#   secretKey: \"\"\n\n# Enables selected built-in secrets that are set via environment variables by default.\n# Those secrets are provided by the Helm Chart secrets by default but in some cases you\n# might want to provide some of those variables with _CMD or _SECRET variable, and you should\n# in this case disable setting of those variables by setting the relevant configuration to false.\nenableBuiltInSecretEnvVars:\n  AIRFLOW__CORE__FERNET_KEY: true\n  # For Airflow \u003c2.3, backward compatibility; moved to [database] in 2.3\n  AIRFLOW__CORE__SQL_ALCHEMY_CONN: true\n  AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: true\n  AIRFLOW_CONN_AIRFLOW_DB: true\n  AIRFLOW__WEBSERVER__SECRET_KEY: true\n  AIRFLOW__CELERY__CELERY_RESULT_BACKEND: true\n  AIRFLOW__CELERY__RESULT_BACKEND: true\n  AIRFLOW__CELERY__BROKER_URL: true\n  AIRFLOW__ELASTICSEARCH__HOST: true\n  AIRFLOW__ELASTICSEARCH__ELASTICSEARCH_HOST: true\n\n# Priority Classes that will be installed by charts.\n# Ideally, there should be an entry for dagProcessor, flower,\n#   pgbouncer, scheduler, statsd, triggerer, webserver, worker.\n# The format for priorityClasses is an array with each element having:\n#   * name is the name of the priorityClass. Ensure the same name is given to the respective section as well\n#   * preemptionPolicy for the priorityClass\n#   * value is the preemption value for the priorityClass\npriorityClasses: []\n#  - name: class1 (if this is for dagProcessor, ensure overriding .Values.dagProcessor.priorityClass too)\n#    preemptionPolicy: PreemptLowerPriority\n#    value: 10000\n#  - name: class2\n#    preemptionPolicy: Never\n#    value: 100000\n\n# Extra secrets that will be managed by the chart\n# (You can use them with extraEnv or extraEnvFrom or some of the extraVolumes values).\n# The format for secret data is \"key/value\" where\n#    * key (can be templated) is the name of the secret that will be created\n#    * value: an object with the standard 'data' or 'stringData' key (or both).\n#          The value associated with those keys must be a string (can be templated)\nextraSecrets: {}\n# eg:\n# extraSecrets:\n#   '{{ .Release.Name }}-airflow-connections':\n#     type: 'Opaque'\n#     labels:\n#       my.custom.label/v1: my_custom_label_value_1\n#     data: |\n#       AIRFLOW_CONN_GCP: 'base64_encoded_gcp_conn_string'\n#       AIRFLOW_CONN_AWS: 'base64_encoded_aws_conn_string'\n#     stringData: |\n#       AIRFLOW_CONN_OTHER: 'other_conn'\n#   '{{ .Release.Name }}-other-secret-name-suffix':\n#     data: |\n#        ...\n#   'proxy-config':\n#     stringData: |\n#        HTTP_PROXY: http://proxy_user:proxy_password@192.168.0.10:2080\n#        HTTPS_PROXY: http://proxy_user:proxy_password@192.168.0.10:2080\n#        NO_PROXY: \"localhost,127.0.0.1,.svc.cluster.local,kubernetes.default.svc\"\n\n# Extra ConfigMaps that will be managed by the chart\n# (You can use them with extraEnv or extraEnvFrom or some of the extraVolumes values).\n# The format for configmap data is \"key/value\" where\n#    * key (can be templated) is the name of the configmap that will be created\n#    * value: an object with the standard 'data' key.\n#          The value associated with this keys must be a string (can be templated)\nextraConfigMaps: {}\n# eg:\n# extraConfigMaps:\n#   '{{ .Release.Name }}-airflow-variables':\n#     labels:\n#       my.custom.label/v2: my_custom_label_value_2\n#     data: |\n#       AIRFLOW_VAR_HELLO_MESSAGE: \"Hi!\"\n#       AIRFLOW_VAR_KUBERNETES_NAMESPACE: \"{{ .Release.Namespace }}\"\n\n# Extra env 'items' that will be added to the definition of airflow containers\n# a string is expected (can be templated).\n# TODO: difference from `env`? This is a templated string. Probably should template `env` and remove this.\nextraEnv: ~\n# eg:\n# extraEnv: |\n#   - name: AIRFLOW__CORE__LOAD_EXAMPLES\n#     value: 'True'\n\n# Extra envFrom 'items' that will be added to the definition of airflow containers\n# A string is expected (can be templated).\nextraEnvFrom: ~\n# eg:\n# extraEnvFrom: |\n#   - secretRef:\n#       name: '{{ .Release.Name }}-airflow-connections'\n#   - configMapRef:\n#       name: '{{ .Release.Name }}-airflow-variables'\n\n# Airflow database \u0026 redis config\ndata:\n  # If secret names are provided, use those secrets\n  # These secrets must be created manually, eg:\n  #\n  # kind: Secret\n  # apiVersion: v1\n  # metadata:\n  #   name: custom-airflow-metadata-secret\n  # type: Opaque\n  # data:\n  #   connection: base64_encoded_connection_string\n\n  metadataSecretName: ~\n  # When providing secret names and using the same database for metadata and\n  # result backend, for Airflow \u003c 2.4.0 it is necessary to create a separate\n  # secret for result backend but with a db+ scheme prefix.\n  # For Airflow \u003e= 2.4.0 it is possible to not specify the secret again,\n  # as Airflow will use sql_alchemy_conn with a db+ scheme prefix by default.\n  resultBackendSecretName: ~\n  brokerUrlSecretName: ~\n\n  # Otherwise pass connection values in\n  metadataConnection:\n    user: postgres\n    pass: postgres\n    protocol: postgresql\n    host: ~\n    port: 5432\n    db: postgres\n    sslmode: disable\n  # resultBackendConnection defaults to the same database as metadataConnection\n  resultBackendConnection: ~\n  # or, you can use a different database\n  # resultBackendConnection:\n  #   user: postgres\n  #   pass: postgres\n  #   protocol: postgresql\n  #   host: ~\n  #   port: 5432\n  #   db: postgres\n  #   sslmode: disable\n  # Note: brokerUrl can only be set during install, not upgrade\n  brokerUrl: ~\n\n# Fernet key settings\n# Note: fernetKey can only be set during install, not upgrade\nfernetKey: ~\nfernetKeySecretName: ~\n\n# Flask secret key for Airflow Webserver: `[webserver] secret_key` in airflow.cfg\nwebserverSecretKey: ~\nwebserverSecretKeySecretName: ~\n\n# In order to use kerberos you need to create secret containing the keytab file\n# The secret name should follow naming convention of the application where resources are\n# name {{ .Release-name }}-\u003cPOSTFIX\u003e. In case of the keytab file, the postfix is \"kerberos-keytab\"\n# So if your release is named \"my-release\" the name of the secret should be \"my-release-kerberos-keytab\"\n#\n# The Keytab content should be available in the \"kerberos.keytab\" key of the secret.\n#\n#  apiVersion: v1\n#  kind: Secret\n#  data:\n#    kerberos.keytab: \u003cbase64_encoded keytab file content\u003e\n#  type: Opaque\n#\n#\n#  If you have such keytab file you can do it with similar\n#\n#  kubectl create secret generic {{ .Release.name }}-kerberos-keytab --from-file=kerberos.keytab\n#\n#\n#  Alternatively, instead of manually creating the secret, it is possible to specify\n#  kerberos.keytabBase64Content parameter. This parameter should contain base64 encoded keytab.\n#\n\nkerberos:\n  enabled: false\n  ccacheMountPath: /var/kerberos-ccache\n  ccacheFileName: cache\n  configPath: /etc/krb5.conf\n  keytabBase64Content: ~\n  keytabPath: /etc/airflow.keytab\n  principal: airflow@FOO.COM\n  reinitFrequency: 3600\n  config: |\n    # This is an example config showing how you can use templating and how \"example\" config\n    # might look like. It works with the test kerberos server that we are using during integration\n    # testing at Apache Airflow (see `scripts/ci/docker-compose/integration-kerberos.yml` but in\n    # order to make it production-ready you must replace it with your own configuration that\n    # Matches your kerberos deployment. Administrators of your Kerberos instance should\n    # provide the right configuration.\n\n    [logging]\n    default = \"FILE:{{ template \"airflow_logs_no_quote\" . }}/kerberos_libs.log\"\n    kdc = \"FILE:{{ template \"airflow_logs_no_quote\" . }}/kerberos_kdc.log\"\n    admin_server = \"FILE:{{ template \"airflow_logs_no_quote\" . }}/kadmind.log\"\n\n    [libdefaults]\n    default_realm = FOO.COM\n    ticket_lifetime = 10h\n    renew_lifetime = 7d\n    forwardable = true\n\n    [realms]\n    FOO.COM = {\n      kdc = kdc-server.foo.com\n      admin_server = admin_server.foo.com\n    }\n\n# Airflow Worker Config\nworkers:\n  # Number of airflow celery workers in StatefulSet\n  replicas: 1\n  # Max number of old replicasets to retain\n  revisionHistoryLimit: ~\n\n  # Command to use when running Airflow workers (templated).\n  command: ~\n  # Args to use when running Airflow workers (templated).\n  args:\n    - \"bash\"\n    - \"-c\"\n    # The format below is necessary to get `helm lint` happy\n    - |-\n      exec \\\n      airflow {{ semverCompare \"\u003e=2.0.0\" .Values.airflowVersion | ternary \"celery worker\" \"worker\" }}\n\n  # If the worker stops responding for 5 minutes (5*60s) kill the\n  # worker and let Kubernetes restart it\n  livenessProbe:\n    enabled: true\n    initialDelaySeconds: 10\n    timeoutSeconds: 20\n    failureThreshold: 5\n    periodSeconds: 60\n    command: ~\n\n  # Update Strategy when worker is deployed as a StatefulSet\n  updateStrategy: ~\n  # Update Strategy when worker is deployed as a Deployment\n  strategy:\n    rollingUpdate:\n      maxSurge: \"100%\"\n      maxUnavailable: \"50%\"\n\n  # When not set, the values defined in the global securityContext will be used\n  securityContext: {}\n  #  runAsUser: 50000\n  #  fsGroup: 0\n  #  runAsGroup: 0\n\n  # Detailed default security context for worker deployments for container and pod level\n  securityContexts:\n    pod: {}\n    container: {}\n\n  # container level lifecycle hooks\n  containerLifecycleHooks: {}\n\n  # Create ServiceAccount\n  serviceAccount:\n    # default value is true\n    # ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/\n    automountServiceAccountToken: true\n    # Specifies whether a ServiceAccount should be created\n    create: true\n    # The name of the ServiceAccount to use.\n    # If not set and create is true, a name is generated using the release name\n    name: ~\n\n    # Annotations to add to worker kubernetes service account.\n    annotations: {}\n\n  # Allow KEDA autoscaling.\n  keda:\n    enabled: false\n    namespaceLabels: {}\n\n    # How often KEDA polls the airflow DB to report new scale requests to the HPA\n    pollingInterval: 5\n\n    # How many seconds KEDA will wait before scaling to zero.\n    # Note that HPA has a separate cooldown period for scale-downs\n    cooldownPeriod: 30\n\n    # Minimum number of workers created by keda\n    minReplicaCount: 0\n\n    # Maximum number of workers created by keda\n    maxReplicaCount: 10\n\n    # Specify HPA related options\n    advanced: {}\n    # horizontalPodAutoscalerConfig:\n    #   behavior:\n    #     scaleDown:\n    #       stabilizationWindowSeconds: 300\n    #       policies:\n    #         - type: Percent\n    #           value: 100\n    #           periodSeconds: 15\n\n    # Query to use for KEDA autoscaling. Must return a single integer.\n    query: \u003e-\n      SELECT ceil(COUNT(*)::decimal / {{ .Values.config.celery.worker_concurrency }})\n      FROM task_instance\n      WHERE (state='running' OR state='queued')\n      {{- if eq .Values.executor \"CeleryKubernetesExecutor\" }}\n      AND queue != '{{ .Values.config.celery_kubernetes_executor.kubernetes_queue }}'\n      {{- end }}\n\n    # Weather to use PGBouncer to connect to the database or not when it is enabled\n    # This configuration will be ignored if PGBouncer is not enabled\n    usePgbouncer: true\n\n  # Allow HPA (KEDA must be disabled).\n  hpa:\n    enabled: false\n\n    # Minimum number of workers created by HPA\n    minReplicaCount: 0\n\n    # Maximum number of workers created by HPA\n    maxReplicaCount: 5\n\n    # Specifications for which to use to calculate the desired replica count\n    metrics:\n      - type: Resource\n        resource:\n          name: cpu\n          target:\n            type: Utilization\n            averageUtilization: 80\n\n    # Scaling behavior of the target in both Up and Down directions\n    behavior: {}\n\n  persistence:\n    # Enable persistent volumes\n    enabled: true\n    # This policy determines whether PVCs should be deleted when StatefulSet is scaled down or removed.\n    persistentVolumeClaimRetentionPolicy: ~\n    # persistentVolumeClaimRetentionPolicy:\n    #   whenDeleted: Delete\n    #   whenScaled: Delete\n    # Volume size for worker StatefulSet\n    size: 1Gi\n    # If using a custom storageClass, pass name ref to all statefulSets here\n    storageClassName:\n    # Execute init container to chown log directory.\n    # This is currently only needed in kind, due to usage\n    # of local-path provisioner.\n    fixPermissions: false\n    # Annotations to add to worker volumes\n    annotations: {}\n    # Detailed default security context for persistence for container level\n    securityContexts:\n      container: {}\n    # container level lifecycle hooks\n    containerLifecycleHooks: {}\n\n  kerberosSidecar:\n    # Enable kerberos sidecar\n    enabled: false\n    resources: {}\n    #  limits:\n    #   cpu: 100m\n    #   memory: 128Mi\n    #  requests:\n    #   cpu: 100m\n    #   memory: 128Mi\n    # Detailed default security context for kerberosSidecar for container level\n    securityContexts:\n      container: {}\n    # container level lifecycle hooks\n    containerLifecycleHooks: {}\n\n  kerberosInitContainer:\n    # Enable kerberos init container\n    enabled: false\n    resources: {}\n    #  limits:\n    #   cpu: 100m\n    #   memory: 128Mi\n    #  requests:\n    #   cpu: 100m\n    #   memory: 128Mi\n\n\n  resources: {}\n  #  limits:\n  #   cpu: 100m\n  #   memory: 128Mi\n  #  requests:\n  #   cpu: 100m\n  #   memory: 128Mi\n\n  # Grace period for tasks to finish after SIGTERM is sent from kubernetes\n  terminationGracePeriodSeconds: 600\n\n  # This setting tells kubernetes that its ok to evict\n  # when it wants to scale a node down.\n  safeToEvict: false\n\n  # Launch additional containers into worker (templated).\n  # Note: If used with KubernetesExecutor, you are responsible for signaling sidecars to exit when the main\n  # container finishes so Airflow can continue the worker shutdown process!\n  extraContainers: []\n  # Add additional init containers into workers (templated).\n  extraInitContainers: []\n\n  # Mount additional volumes into worker. It can be templated like in the following example:\n  #   extraVolumes:\n  #     - name: my-templated-extra-volume\n  #       secret:\n  #          secretName: '{{ include \"my_secret_template\" . }}'\n  #          defaultMode: 0640\n  #          optional: true\n  #\n  #   extraVolumeMounts:\n  #     - name: my-templated-extra-volume\n  #       mountPath: \"{{ .Values.my_custom_path }}\"\n  #       readOnly: true\n  extraVolumes: []\n  extraVolumeMounts: []\n\n  # Select certain nodes for airflow worker pods.\n  nodeSelector: {}\n  runtimeClassName: ~\n  priorityClassName: ~\n  affinity: {}\n  # default worker affinity is:\n  #  podAntiAffinity:\n  #    preferredDuringSchedulingIgnoredDuringExecution:\n  #    - podAffinityTerm:\n  #        labelSelector:\n  #          matchLabels:\n  #            component: worker\n  #        topologyKey: kubernetes.io/hostname\n  #      weight: 100\n  tolerations: []\n  topologySpreadConstraints: []\n  # hostAliases to use in worker pods.\n  # See:\n  # https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/\n  hostAliases: []\n  # - ip: \"127.0.0.2\"\n  #   hostnames:\n  #   - \"test.hostname.one\"\n  # - ip: \"127.0.0.3\"\n  #   hostnames:\n  #   - \"test.hostname.two\"\n\n  # annotations for the worker resource\n  annotations: {}\n\n  podAnnotations: {}\n\n  # Labels specific to workers objects and pods\n  labels: {}\n\n  logGroomerSidecar:\n    # Whether to deploy the Airflow worker log groomer sidecar.\n    enabled: true\n    # Command to use when running the Airflow worker log groomer sidecar (templated).\n    command: ~\n    # Args to use when running the Airflow worker log groomer sidecar (templated).\n    args: [\"bash\", \"/clean-logs\"]\n    # Number of days to retain logs\n    retentionDays: 15\n    resources: {}\n    #  limits:\n    #   cpu: 100m\n    #   memory: 128Mi\n    #  requests:\n    #   cpu: 100m\n    #   memory: 128Mi\n    # Detailed default security context for logGroomerSidecar for container level\n    securityContexts:\n      container: {}\n\n  waitForMigrations:\n    # Whether to create init container to wait for db migrations\n    enabled: true\n    env: []\n    # Detailed default security context for waitForMigrations for container level\n    securityContexts:\n      container: {}\n\n  env: []\n\n  volumeClaimTemplates: []\n  # Additional volumeClaimTemplates needed.\n  # Comment out the above and uncomment the section below to enable it.\n  # Add more as needed\n  # Make sure to mount it under extraVolumeMounts.\n  # volumeClaimTemplates:\n  #   - metadata:\n  #       name: data-volume-1\n  #     spec:\n  #       storageClassName: \"storage-class-1\"\n  #       accessModes:\n  #         - \"ReadWriteOnce\"\n  #       resources:\n  #         requests:\n  #           storage: \"10Gi\"\n  #   - metadata:\n  #       name: data-volume-2\n  #     spec:\n  #       storageClassName: \"storage-class-2\"\n  #       accessModes:\n  #         - \"ReadWriteOnce\"\n  #       resources:\n  #         requests:\n  #           storage: \"20Gi\"\n\n# Airflow scheduler settings\nscheduler:\n  enabled: true\n  #  hostAliases for the scheduler pod\n  hostAliases: []\n  #  - ip: \"127.0.0.1\"\n  #    hostnames:\n  #      - \"foo.local\"\n  #  - ip: \"10.1.2.3\"\n  #    hostnames:\n  #      - \"foo.remote\"\n\n  # If the scheduler stops heartbeating for 5 minutes (5*60s) kill the\n  # scheduler and let Kubernetes restart it\n  livenessProbe:\n    initialDelaySeconds: 10\n    timeoutSeconds: 20\n    failureThreshold: 5\n    periodSeconds: 60\n    command: ~\n\n  # Wait for at most 1 minute (6*10s) for the scheduler container to startup.\n  # livenessProbe kicks in after the first successful startupProbe\n  startupProbe:\n    failureThreshold: 6\n    periodSeconds: 10\n    timeoutSeconds: 20\n    command: ~\n\n  # Airflow 2.0 allows users to run multiple schedulers,\n  # However this feature is only recommended for MySQL 8+ and Postgres\n  replicas: 1\n  # Max number of old replicasets to retain\n  revisionHistoryLimit: ~\n\n  # Command to use when running the Airflow scheduler (templated).\n  command: ~\n  # Args to use when running the Airflow scheduler (templated).\n  args: [\"bash\", \"-c\", \"exec airflow scheduler\"]\n\n  # Update Strategy when scheduler is deployed as a StatefulSet\n  # (when using LocalExecutor and workers.persistence)\n  updateStrategy: ~\n  # Update Strategy when scheduler is deployed as a Deployment\n  # (when not using LocalExecutor and workers.persistence)\n  strategy: ~\n\n  # When not set, the values defined in the global securityContext will be used\n  # (deprecated, use `securityContexts` instead)\n  securityContext: {}\n  #  runAsUser: 50000\n  #  fsGroup: 0\n  #  runAsGroup: 0\n\n  # Detailed default security context for scheduler deployments for container and pod level\n  securityContexts:\n    pod: {}\n    container: {}\n\n  # container level lifecycle hooks\n  containerLifecycleHooks: {}\n\n  # Create ServiceAccount\n  serviceAccount:\n    # default value is true\n    # ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/\n    automountServiceAccountToken: true\n    # Specifies whether a ServiceAccount should be created\n    create: true\n    # The name of the ServiceAccount to use.\n    # If not set and create is true, a name is generated using the release name\n    name: ~\n\n    # Annotations to add to scheduler kubernetes service account.\n    annotations: {}\n\n  # Scheduler pod disruption budget\n  podDisruptionBudget:\n    enabled: false\n\n    # PDB configuration\n    config:\n      # minAvailable and maxUnavailable are mutually exclusive\n      maxUnavailable: 1\n      # minAvailable: 1\n\n  resources: {}\n  #  limits:\n  #   cpu: 100m\n  #   memory: 128Mi\n  #  requests:\n  #   cpu: 100m\n  #   memory: 128Mi\n\n  # This setting tells kubernetes that its ok to evict\n  # when it wants to scale a node down.\n  safeToEvict: true\n\n  # Launch additional containers into scheduler (templated).\n  extraContainers: []\n  # Add additional init containers into scheduler (templated).\n  extraInitContainers: []\n\n  # Mount additional volumes into scheduler. It can be templated like in the following example:\n  #   extraVolumes:\n  #     - name: my-templated-extra-volume\n  #       secret:\n  #          secretName: '{{ include \"my_secret_template\" . }}'\n  #          defaultMode: 0640\n  #          optional: true\n  #\n  #   extraVolumeMounts:\n  #     - name: my-templated-extra-volume\n  #       mountPath: \"{{ .Values.my_custom_path }}\"\n  #       readOnly: true\n  extraVolumes: []\n  extraVolumeMounts: []\n\n  # Select certain nodes for airflow scheduler pods.\n  nodeSelector: {}\n  affinity: {}\n  # default scheduler affinity is:\n  #  podAntiAffinity:\n  #    preferredDuringSchedulingIgnoredDuringExecution:\n  #    - podAffinityTerm:\n  #        labelSelector:\n  #          matchLabels:\n  #            component: scheduler\n  #        topologyKey: kubernetes.io/hostname\n  #      weight: 100\n  tolerations: []\n  topologySpreadConstraints: []\n\n  priorityClassName: ~\n\n  # annotations for scheduler deployment\n  annotations: {}\n\n  podAnnotations: {}\n\n  # Labels specific to scheduler objects and pods\n  labels: {}\n\n  logGroomerSidecar:\n    # Whether to deploy the Airflow scheduler log groomer sidecar.\n    enabled: true\n    # Command to use when running the Airflow scheduler log groomer sidecar (templated).\n    command: ~\n    # Args to use when running the Airflow scheduler log groomer sidecar (templated).\n    args: [\"bash\", \"/clean-logs\"]\n    # Number of days to retain logs\n    retentionDays: 15\n    resources: {}\n    #  limits:\n    #   cpu: 100m\n    #   memory: 128Mi\n    #  requests:\n    #   cpu: 100m\n    #   memory: 128Mi\n    # Detailed default security context for logGroomerSidecar for container level\n    securityContexts:\n      container: {}\n    # container level lifecycle hooks\n    containerLifecycleHooks: {}\n\n  waitForMigrations:\n    # Whether to create init container to wait for db migrations\n    enabled: true\n    env: []\n    # Detailed default security context for waitForMigrations for container level\n    securityContexts:\n      container: {}\n\n  env: []\n\n# Airflow create user job settings\ncreateUserJob:\n  # Limit the lifetime of the job object after it finished execution.\n  ttlSecondsAfterFinished: 300\n  # Command to use when running the create user job (templated).\n  command: ~\n  # Args to use when running the create user job (templated).\n  args:\n    - \"bash\"\n    - \"-c\"\n    # The format below is necessary to get `helm lint` happy\n    - |-\n      exec \\\n      airflow {{ semverCompare \"\u003e=2.0.0\" .Values.airflowVersion | ternary \"users create\" \"create_user\" }} \"$@\"\n    - --\n    - \"-r\"\n    - \"{{ .Values.webserver.defaultUser.role }}\"\n    - \"-u\"\n    - \"{{ .Values.webserver.defaultUser.username }}\"\n    - \"-e\"\n    - \"{{ .Values.webserver.defaultUser.email }}\"\n    - \"-f\"\n    - \"{{ .Values.webserver.defaultUser.firstName }}\"\n    - \"-l\"\n    - \"{{ .Values.webserver.defaultUser.lastName }}\"\n    - \"-p\"\n    - \"{{ .Values.webserver.defaultUser.password }}\"\n\n  # Annotations on the create user job pod\n  annotations: {}\n  # jobAnnotations are annotations on the create user job\n  jobAnnotations: {}\n\n  # Labels specific to createUserJob objects and pods\n  labels: {}\n\n  # When not set, the values defined in the global securityContext will be used\n  securityContext: {}\n  #  runAsUser: 50000\n  #  fsGroup: 0\n  #  runAsGroup: 0\n\n  # Detailed default security context for createUserJob for container and pod level\n  securityContexts:\n    pod: {}\n    container: {}\n\n  # container level lifecycle hooks\n  containerLifecycleHooks: {}\n\n  # Create ServiceAccount\n  serviceAccount:\n    # default value is true\n    # ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/\n    automountServiceAccountToken: true\n    # Specifies whether a ServiceAccount should be created\n    create: true\n    # The name of the ServiceAccount to use.\n    # If not set and create is true, a name is generated using the release name\n    name: ~\n\n    # Annotations to add to create user kubernetes service account.\n    annotations: {}\n\n  # Launch additional containers into user creation job\n  extraContainers: []\n\n  # Add additional init containers into user creation job (templated).\n  extraInitContainers: []\n\n  # Mount additional volumes into user creation job. It can be templated like in the following example:\n  #   extraVolumes:\n  #     - name: my-templated-extra-volume\n  #       secret:\n  #          secretName: '{{ include \"my_secret_template\" . }}'\n  #          defaultMode: 0640\n  #          optional: true\n  #\n  #   extraVolumeMounts:\n  #     - name: my-templated-extra-volume\n  #       mountPath: \"{{ .Values.my_custom_path }}\"\n  #       readOnly: true\n  extraVolumes: []\n  extraVolumeMounts: []\n\n  nodeSelector: {}\n  affinity: {}\n  tolerations: []\n  topologySpreadConstraints: []\n  priorityClassName: ~\n  # In case you need to disable the helm hooks that create the jobs after install.\n  # Disable this if you are using ArgoCD for example\n  useHelmHooks: true\n  applyCustomEnv: true\n\n  env: []\n\n  resources: {}\n  #  limits:\n  #   cpu: 100m\n  #   memory: 128Mi\n  #  requests:\n  #   cpu: 100m\n  #   memory: 128Mi\n\n# Airflow database migration job settings\nmigrateDatabaseJob:\n  enabled: true\n  # Limit the lifetime of the job object after it finished execution.\n  ttlSecondsAfterFinished: 300\n  # Command to use when running the migrate database job (templated).\n  command: ~\n  # Args to use when running the migrate database job (templated).\n  args:\n    - \"bash\"\n    - \"-c\"\n    - \u003e-\n      exec \\\n\n      airflow {{ semverCompare \"\u003e=2.7.0\" .Values.airflowVersion\n      | ternary \"db migrate\" (semverCompare \"\u003e=2.0.0\" .Values.airflowVersion\n      | ternary \"db upgrade\" \"upgradedb\") }}\n\n  # Annotations on the database migration pod\n  annotations: {}\n  # jobAnnotations are annotations on the database migration job\n  jobAnnotations: {}\n\n  # Labels specific to migrate database job objects and pods\n  labels: {}\n\n  # When not set, the values defined in the global securityContext will be used\n  securityContext: {}\n  #  runAsUser: 50000\n  #  fsGroup: 0\n  #  runAsGroup: 0\n\n  # Detailed default security context for migrateDatabaseJob for container and pod level\n  securityContexts:\n    pod: {}\n    container: {}\n\n  # container level lifecycle hooks\n  containerLifecycleHooks: {}\n\n  # Create ServiceAccount\n  serviceAccount:\n    # default value is true\n    # ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/\n    automountServiceAccountToken: true\n    # Specifies whether a ServiceAccount should be created\n    create: true\n    # The name of the ServiceAccount to use.\n    # If not set and create is true, a name is generated using the release name\n    name: ~\n\n    # Annotations to add to migrate database job kubernetes service account.\n    annotations: {}\n\n  resources: {}\n  #  limits:\n  #   cpu: 100m\n  #   memory: 128Mi\n  #  requests:\n  #   cpu: 100m\n  #   memory: 128Mi\n\n  # Launch additional containers into database migration job\n  extraContainers: []\n\n  # Add additional init containers into migrate database job (templated).\n  extraInitContainers: []\n\n  # Mount additional volumes into database migration job. It can be templated like in the following example:\n  #   extraVolumes:\n  #     - name: my-templated-extra-volume\n  #       secret:\n  #          secretName: '{{ include \"my_secret_template\" . }}'\n  #          defaultMode: 0640\n  #          optional: true\n  #\n  #   extraVolumeMounts:\n  #     - name: my-templated-extra-volume\n  #       mountPath: \"{{ .Values.my_custom_path }}\"\n  #       readOnly: true\n  extraVolumes: []\n  extraVolumeMounts: []\n\n  nodeSelector: {}\n  affinity: {}\n  tolerations: []\n  topologySpreadConstraints: []\n  priorityClassName: ~\n  # In case you need to disable the helm hooks that create the jobs after install.\n  # Disable this if you are using ArgoCD for example\n  useHelmHooks: true\n  applyCustomEnv: true\n\n# rpcServer support is experimental / dev purpose only and will later be renamed\n_rpcServer:\n  enabled: false\n\n  # Labels specific to workers objects and pods\n  labels: {}\n\n  # Command to use when running the Airflow rpc server (templated).\n  command:\n    - \"bash\"\n  # Args to use when running the Airflow rpc server (templated).\n  args: [\"-c\", \"exec airflow internal-api\"]\n  env: []\n  serviceAccount:\n    # default value is true\n    # ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/\n    automountServiceAccountToken: true\n    # Specifies whether a ServiceAccount should be created\n    create: true\n    # The name of the ServiceAccount to use.\n    # If not set and create is true, a name is generated using the release name\n    name: ~\n\n    # Annotations to add to webserver kubernetes service account.\n    annotations: {}\n  service:\n    type: ClusterIP\n    ## service annotations\n    annotations: {}\n    ports:\n      - name: rpc-server\n        port: \"{{ .Values.ports._rpcServer }}\"\n\n    loadBalancerIP: ~\n    ## Limit load balancer source ips to list of CIDRs\n    # loadBalancerSourceRanges:\n    #   - \"10.123.0.0/16\"\n    loadBalancerSourceRanges: []\n\n  podDisruptionBudget:\n    enabled: false\n\n    # PDB configuration\n    config:\n      # minAvailable and maxUnavailable are mutually exclusive\n      maxUnavailable: 1\n      # minAvailable: 1\n\n  # Detailed default security contexts for webserver deployments for container and pod level\n  securityContexts:\n    pod: {}\n    container: {}\n\n  waitForMigrations:\n    # Whether to create init container to wait for db migrations\n    enabled: true\n    env: []\n    # Detailed default security context for waitForMigrations for container level\n    securityContexts:\n      container: {}\n\n  # Launch additional containers into the flower pods.\n  extraContainers: []\n\n  # Additional network policies as needed (Deprecated - renamed to `webserver.networkPolicy.ingress.from`)\n  extraNetworkPolicies: []\n  networkPolicy:\n    ingress:\n      # Peers for webserver NetworkPolicy ingress\n      from: []\n      # Ports for webserver NetworkPolicy ingress (if `from` is set)\n      ports:\n        - port: \"{{ .Values.ports._rpcServer }}\"\n\n  resources: {}\n  #   limits:\n  #     cpu: 100m\n  #     memory: 128Mi\n  #   requests:\n  #     cpu: 100m\n  #     memory: 128Mi\n\n  livenessProbe:\n    initialDelaySeconds: 15\n    timeoutSeconds: 5\n    failureThreshold: 5\n    periodSeconds: 10\n    scheme: HTTP\n\n  readinessProbe:\n    initialDelaySeconds: 15\n    timeoutSeconds: 5\n    failureThreshold: 5\n    periodSeconds: 10\n    scheme: HTTP\n\n  # Wait for at most 1 minute (6*10s) for the RPC server container to startup.\n  # livenessProbe kicks in after the first successful startupProbe\n  startupProbe:\n    timeoutSeconds: 20\n    failureThreshold: 6\n    periodSeconds: 10\n    scheme: HTTP\n\n# Airflow webserver settings\nwebserver:\n  enabled: true\n  # Add custom annotations to the webserver configmap\n  configMapAnnotations: {}\n  #  hostAliases for the webserver pod\n  hostAliases: []\n  #  - ip: \"127.0.0.1\"\n  #    hostnames:\n  #      - \"foo.local\"\n  #  - ip: \"10.1.2.3\"\n  #    hostnames:\n  #      - \"foo.remote\"\n  allowPodLogReading: true\n  livenessProbe:\n    initialDelaySeconds: 15\n    timeoutSeconds: 5\n    failureThreshold: 5\n    periodSeconds: 10\n    scheme: HTTP\n\n  readinessProbe:\n    initialDelaySeconds: 15\n    timeoutSeconds: 5\n    failureThreshold: 5\n    periodSeconds: 10\n    scheme: HTTP\n\n  # Wait for at most 1 minute (6*10s) for the webserver container to startup.\n  # livenessProbe kicks in after the first successful startupProbe\n  startupProbe:\n    timeoutSeconds: 20\n    failureThreshold: 6\n    periodSeconds: 10\n    scheme: HTTP\n\n  # Number of webservers\n  replicas: 1\n  # Max number of old replicasets to retain\n  revisionHistoryLimit: ~\n\n  # Command to use when running the Airflow webserver (templated).\n  command: ~\n  # Args to use when running the Airflow webserver (templated).\n  args: [\"bash\", \"-c\", \"exec airflow webserver\"]\n\n  # Create ServiceAccount\n  serviceAccount:\n    # default value is true\n    # ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/\n    automountServiceAccountToken: true\n    # Specifies whether a ServiceAccount should be created\n    create: true\n    # The name of the ServiceAccount to use.\n    # If not set and create is true, a name is generated using the release name\n    name: ~\n\n    # Annotations to add to webserver kubernetes service account.\n    annotations: {}\n\n  # Webserver pod disruption budget\n  podDisruptionBudget:\n    enabled: false\n\n    # PDB configuration\n    config:\n      # minAvailable and maxUnavailable are mutually exclusive\n      maxUnavailable: 1\n      # minAvailable: 1\n\n  # Allow overriding Update Strategy for Webserver\n  strategy: ~\n\n  # When not set, the values defined in the global securityContext will be used\n  # (deprecated, use `securityContexts` instead)\n  securityContext: {}\n  #  runAsUser: 50000\n  #  fsGroup: 0\n  #  runAsGroup: 0\n\n  # Detailed default security contexts for webserver deployments for container and pod level\n  securityContexts:\n    pod: {}\n    container: {}\n\n  # container level lifecycle hooks\n  containerLifecycleHooks: {}\n\n  # Additional network policies as needed (Deprecated - renamed to `webserver.networkPolicy.ingress.from`)\n  extraNetworkPolicies: []\n  networkPolicy:\n    ingress:\n      # Peers for webserver NetworkPolicy ingress\n      from: []\n      # Ports for webserver NetworkPolicy ingress (if `from` is set)\n      ports:\n        - port: \"{{ .Values.ports.airflowUI }}\"\n\n  resources: {}\n  #   limits:\n  #     cpu: 100m\n  #     memory: 128Mi\n  #   requests:\n  #     cpu: 100m\n  #     memory: 128Mi\n\n  # Create initial user.\n  defaultUser:\n    enabled: true\n    role: Admin\n    username: admin\n    email: admin@example.com\n    firstName: admin\n    lastName: user\n    password: admin\n\n  # Launch additional containers into webserver (templated).\n  extraContainers: []\n  # Add additional init containers into webserver (templated).\n  extraInitContainers: []\n\n  # Mount additional volumes into webserver. It can be templated like in the following example:\n  #   extraVolumes:\n  #     - name: my-templated-extra-volume\n  #       secret:\n  #          secretName: '{{ include \"my_secret_template\" . }}'\n  #          defaultMode: 0640\n  #          optional: true\n  #\n  #   extraVolumeMounts:\n  #     - name: my-templated-extra-volume\n  #       mountPath: \"{{ .Values.my_custom_path }}\"\n  #       readOnly: true\n  extraVolumes: []\n  extraVolumeMounts: []\n\n  # This string (can be templated) will be mounted into the Airflow Webserver\n  # as a custom webserver_config.py. You can bake a webserver_config.py in to\n  # your image instead or specify a configmap containing the\n  # webserver_config.py.\n  webserverConfig: ~\n  # webserverConfig: |\n  #   from airflow import configuration as conf\n\n  #   # The SQLAlchemy connection string.\n  #   SQLALCHEMY_DATABASE_URI = conf.get('database', 'SQL_ALCHEMY_CONN')\n\n  #   # Flask-WTF flag for CSRF\n  #   CSRF_ENABLED = True\n  webserverConfigConfigMapName: ~\n\n  service:\n    type: ClusterIP\n    ## service annotations\n    annotations: {}\n    ports:\n      - name: airflow-ui\n        port: \"{{ .Values.ports.airflowUI }}\"\n    # To change the port used to access the webserver:\n    # ports:\n    #   - name: airflow-ui\n    #     port: 80\n    #     targetPort: airflow-ui\n    # To only expose a sidecar, not the webserver directly:\n    # ports:\n    #   - name: only_sidecar\n    #     port: 80\n    #     targetPort: 8888\n    # If you have a public IP, set NodePort to set an external port.\n    # Service type must be 'NodePort':\n    # ports:\n    #   - name: airflow-ui\n    #     port: 8080\n    #     targetPort: 8080\n    #     nodePort: 31151\n    loadBalancerIP: ~\n    ## Limit load balancer source ips to list of CIDRs\n    # loadBalancerSourceRanges:\n    #   - \"10.123.0.0/16\"\n    loadBalancerSourceRanges: []\n\n  # Select certain nodes for airflow webserver pods.\n  nodeSelector: {}\n  priorityClassName: ~\n  affinity: {}\n  # default webserver affinity is:\n  #  podAntiAffinity:\n  #    preferredDuringSchedulingIgnoredDuringExecution:\n  #    - podAffinityTerm:\n  #        labelSelector:\n  #          matchLabels:\n  #            component: webserver\n  #        topologyKey: kubernetes.io/hostname\n  #      weight: 100\n  tolerations: []\n  topologySpreadConstraints: []\n\n  # annotations for webserver deployment\n  annotations: {}\n\n  podAnnotations: {}\n\n  # Labels specific webserver app\n  labels: {}\n\n  waitForMigrations:\n    # Whether to create init container to wait for db migrations\n    enabled: true\n    env: []\n    # Detailed default security context for waitForMigrations for container level\n    securityContexts:\n      container: {}\n\n  env: []\n\n# Airflow Triggerer Config\ntriggerer:\n  enabled: true\n  # Number of airflow triggerers in the deployment\n  replicas: 1\n  # Max number of old replicasets to retain\n  revisionHistoryLimit: ~\n\n  # Command to use when running Airflow triggerers (templated).\n  command: ~\n  # Args to use when running Airflow triggerer (templated).\n  args: [\"bash\", \"-c\", \"exec airflow triggerer\"]\n\n  # Update Strategy when triggerer is deployed as a StatefulSet\n  updateStrategy: ~\n  # Update Strategy when triggerer is deployed as a Deployment\n  strategy:\n    rollingUpdate:\n      maxSurge: \"100%\"\n      maxUnavailable: \"50%\"\n\n  # If the triggerer stops heartbeating for 5 minutes (5*60s) kill the\n  # triggerer and let Kubernetes restart it\n  livenessProbe:\n    initialDelaySeconds: 10\n    timeoutSeconds: 20\n    failureThreshold: 5\n    periodSeconds: 60\n    command: ~\n\n  # Create ServiceAccount\n  serviceAccount:\n    # default value is true\n    # ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/\n    automountServiceAccountToken: true\n    # Specifies whether a ServiceAccount should be created\n    create: true\n    # The name of the ServiceAccount to use.\n    # If not set and create is true, a name is generated using the release name\n    name: ~\n\n    # Annotations to add to triggerer kubernetes service account.\n    annotations: {}\n\n  # When not set, the values defined in the global securityContext will be used\n  securityContext: {}\n  #  runAsUser: 50000\n  #  fsGroup: 0\n  #  runAsGroup: 0\n\n  # Detailed default security context for triggerer for container and pod level\n  securityContexts:\n    pod: {}\n    container: {}\n\n  # container level lifecycle hooks\n  containerLifecycleHooks: {}\n\n  persistence:\n    # Enable persistent volumes\n    enabled: true\n    # This policy determines whether PVCs should be deleted when StatefulSet is scaled down or removed.\n    persistentVolumeClaimRetentionPolicy: ~\n    # Volume size for triggerer StatefulSet\n    size: 1Gi\n    # If using a custom storageClass, pass name ref to all statefulSets here\n    storageClassName:\n    # Execute init container to chown log directory.\n    # This is currently only needed in kind, due to usage\n    # of local-path provisioner.\n    fixPermissions: false\n    # Annotations to add to triggerer volumes\n    annotations: {}\n\n  resources: {}\n  #  limits:\n  #   cpu: 100m\n  #   memory: 128Mi\n  #  requests:\n  #   cpu: 100m\n  #   memory: 128Mi\n\n  # Grace period for triggerer to finish after SIGTERM is sent from kubernetes\n  terminationGracePeriodSeconds: 60\n\n  # This setting tells kubernetes that its ok to evict\n  # when it wants to scale a node down.\n  safeToEvict: true\n\n  # Launch additional containers into triggerer (templated).\n  extraContainers: []\n  # Add additional init containers into triggerers (templated).\n  extraInitContainers: []\n\n  # Mount additional volumes into triggerer. It can be templated like in the following example:\n  #   extraVolumes:\n  #     - name: my-templated-extra-volume\n  #       secret:\n  #          secretName: '{{ include \"my_secret_template\" . }}'\n  #          defaultMode: 0640\n  #          optional: true\n  #\n  #   extraVolumeMounts:\n  #     - name: my-templated-extra-volume\n  #       mountPath: \"{{ .Values.my_custom_path }}\"\n  #       readOnly: true\n  extraVolumes: []\n  extraVolumeMounts: []\n\n  # Select certain nodes for airflow triggerer pods.\n  nodeSelector: {}\n  affinity: {}\n  # default triggerer affinity is:\n  #  podAntiAffinity:\n  #    preferredDuringSchedulingIgnoredDuringExecution:\n  #    - podAffinityTerm:\n  #        labelSelector:\n  #          matchLabels:\n  #            component: triggerer\n  #        topologyKey: kubernetes.io/hostname\n  #      weight: 100\n  tolerations: []\n  topologySpreadConstraints: []\n\n  priorityClassName: ~\n\n  # annotations for the triggerer deployment\n  annotations: {}\n\n  podAnnotations: {}\n\n  # Labels specific to triggerer objects and pods\n  labels: {}\n\n  logGroomerSidecar:\n    # Whether to deploy the Airflow triggerer log groomer sidecar.\n    enabled: true\n    # Command to use when running the Airflow triggerer log groomer sidecar (templated).\n    command: ~\n    # Args to use when running the Airflow triggerer log groomer sidecar (templated).\n    args: [\"bash\", \"/clean-logs\"]\n    # Number of days to retain logs\n    retentionDays: 15\n    resources: {}\n    #  limits:\n    #   cpu: 100m\n    #   memory: 128Mi\n    #  requests:\n    #   cpu: 100m\n    #   memory: 128Mi\n    # Detailed default security context for logGroomerSidecar for container level\n    securityContexts:\n      container: {}\n\n    # container level lifecycle hooks\n    containerLifecycleHooks: {}\n\n  waitForMigrations:\n    # Whether to create init container to wait for db migrations\n    enabled: true\n    env: []\n    # Detailed default security context for waitForMigrations for container level\n    securityContexts:\n      container: {}\n\n  env: []\n\n  # Allow KEDA autoscaling.\n  keda:\n    enabled: false\n    namespaceLabels: {}\n\n    # How often KEDA polls the airflow DB to report new scale requests to the HPA\n    pollingInterval: 5\n\n    # How many seconds KEDA will wait before scaling to zero.\n    # Note that HPA has a separate cooldown period for scale-downs\n    cooldownPeriod: 30\n\n    # Minimum number of triggerers created by keda\n    minReplicaCount: 0\n\n    # Maximum number of triggerers created by keda\n    maxReplicaCount: 10\n\n    # Specify HPA related options\n    advanced: {}\n    # horizontalPodAutoscalerConfig:\n    #   behavior:\n    #     scaleDown:\n    #       stabilizationWindowSeconds: 300\n    #       policies:\n    #         - type: Percent\n    #           value: 100\n    #           periodSeconds: 15\n\n    # Query to use for KEDA autoscaling. Must return a single integer.\n    query: \u003e-\n      SELECT ceil(COUNT(*)::decimal / {{ .Values.config.triggerer.default_capacity }})\n      FROM trigger\n\n    # Whether to use PGBouncer to connect to the database or not when it is enabled\n    # This configuration will be ignored if PGBouncer is not enabled\n    usePgbouncer: false\n\n# Airflow Dag Processor Config\ndagProcessor:\n  enabled: false\n  # Number of airflow dag processors in the deployment\n  replicas: 1\n  # Max number of old replicasets to retain\n  revisionHistoryLimit: ~\n\n  # Command to use when running Airflow dag processors (templated).\n  command: ~\n  # Args to use when running Airflow dag processor (templated).\n  args: [\"bash\", \"-c\", \"exec airflow dag-processor\"]\n\n  # Update Strategy for dag processors\n  strategy:\n    rollingUpdate:\n      maxSurge: \"100%\"\n      maxUnavailable: \"50%\"\n\n  # If the dag processor stops heartbeating for 5 minutes (5*60s) kill the\n  # dag processor and let Kubernetes restart it\n  livenessProbe:\n    initialDelaySeconds: 10\n    timeoutSeconds: 20\n    failureThreshold: 5\n    periodSeconds: 60\n    command: ~\n\n  # Create ServiceAccount\n  serviceAccount:\n    # default value is true\n    # ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/\n    automountServiceAccountToken: true\n    # Specifies whether a ServiceAccount should be created\n    create: true\n    # The name of the ServiceAccount to use.\n    # If not set and create is true, a name is generated using the release name\n    name: ~\n\n    # Annotations to add to dag processor kubernetes service account.\n    annotations: {}\n\n  # When not set, the values defined in the global securityContext will be used\n  securityContext: {}\n  #  runAsUser: 50000\n  #  fsGroup: 0\n  #  runAsGroup: 0\n\n  # Detailed default security context for dagProcessor for container and pod level\n  securityContexts:\n    pod: {}\n    container: {}\n\n  # container level lifecycle hooks\n  containerLifecycleHooks: {}\n\n  resources: {}\n  #  limits:\n  #   cpu: 100m\n  #   memory: 128Mi\n  #  requests:\n  #   cpu: 100m\n  #   memory: 128Mi\n\n  # Grace period for dag processor to finish after SIGTERM is sent from kubernetes\n  terminationGracePeriodSeconds: 60\n\n  # This setting tells kubernetes that its ok to evict\n  # when it wants to scale a node down.\n  safeToEvict: true\n\n  # Launch additional containers into dag processor (templated).\n  extraContainers: []\n  # Add additional init containers into dag processors (templated).\n  extraInitContainers: []\n\n  # Mount additional volumes into dag processor. It can be templated like in the following example:\n  #   extraVolumes:\n  #     - name: my-templated-extra-volume\n  #       secret:\n  #          secretName: '{{ include \"my_secret_template\" . }}'\n  #          defaultMode: 0640\n  #          optional: true\n  #\n  #   extraVolumeMounts:\n  #     - name: my-templated-extra-volume\n  #       mountPath: \"{{ .Values.my_custom_path }}\"\n  #       readOnly: true\n  extraVolumes: []\n  extraVolumeMounts: []\n\n  # Select certain nodes for airflow dag processor pods.\n  nodeSelector: {}\n  affinity: {}\n  # default dag processor affinity is:\n  #  podAntiAffinity:\n  #    preferredDuringSchedulingIgnoredDuringExecution:\n  #    - podAffinityTerm:\n  #        labelSelector:\n  #          matchLabels:\n  #            component: dag-processor\n  #        topologyKey: kubernetes.io/hostname\n  #      weight: 100\n  tolerations: []\n  topologySpreadConstraints: []\n\n  priorityClassName: ~\n\n  # annotations for the dag processor deployment\n  annotations: {}\n\n  podAnnotations: {}\n\n  logGroomerSidecar:\n    # Whether to deploy the Airflow dag processor log groomer sidecar.\n    enabled: true\n    # Command to use when running the Airflow dag processor log groomer sidecar (templated).\n    command: ~\n    # Args to use when running the Airflow dag processor log groomer sidecar (templated).\n    args: [\"bash\", \"/clean-logs\"]\n    # Number of days to retain logs\n    retentionDays: 15\n    resources: {}\n    #  limits:\n    #   cpu: 100m\n    #   memory: 128Mi\n    #  requests:\n    #   cpu: 100m\n    #   memory: 128Mi\n    securityContexts:\n      container: {}\n\n  waitForMigrations:\n    # Whether to create init container to wait for db migrations\n    enabled: true\n    env: []\n    # Detailed default security context for waitForMigrations for container level\n    securityContexts:\n      container: {}\n\n  env: []\n\n# Flower settings\nflower:\n  # Enable flower.\n  # If True, and using CeleryExecutor/CeleryKubernetesExecutor, will deploy flower app.\n  enabled: false\n\n  livenessProbe:\n    initialDelaySeconds: 10\n    timeoutSeconds: 5\n    failureThreshold: 10\n    periodSeconds: 5\n\n  readinessProbe:\n    initialDelaySeconds: 10\n    timeoutSeconds: 5\n    failureThreshold: 10\n    periodSeconds: 5\n\n  # Max number of old replicasets to retain\n  revisionHistoryLimit: ~\n\n  # Command to use when running flower (templated).\n  command: ~\n  # Args to use when running flower (templated).\n  args:\n    - \"bash\"\n    - \"-c\"\n    # The format below is necessary to get `helm lint` happy\n    - |-\n      exec \\\n      airflow {{ semverCompare \"\u003e=2.0.0\" .Values.airflowVersion | ternary \"celery flower\" \"flower\" }}\n\n  # Additional network policies as needed (Deprecated - renamed to `flower.networkPolicy.ingress.from`)\n  extraNetworkPolicies: []\n  networkPolicy:\n    ingress:\n      # Peers for flower NetworkPolicy ingress\n      from: []\n      # Ports for flower NetworkPolicy ingress (if ingressPeers is set)\n      ports:\n        - port: \"{{ .Values.ports.flowerUI }}\"\n\n  resources: {}\n  #   limits:\n  #     cpu: 100m\n  #     memory: 128Mi\n  #   requests:\n  #     cpu: 100m\n  #     memory: 128Mi\n\n  # When not set, the values defined in the global securityContext will be used\n  securityContext: {}\n  #  runAsUser: 50000\n  #  fsGroup: 0\n  #  runAsGroup: 0\n\n  # Detailed default security context for flower for container and pod level\n  securityContexts:\n    pod: {}\n    container: {}\n\n  # container level lifecycle hooks\n  containerLifecycleHooks: {}\n\n  # Create ServiceAccount\n  serviceAccount:\n    # default value is true\n    # ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/\n    automountServiceAccountToken: true\n    # Specifies whether a ServiceAccount should be created\n    create: true\n    # The name of the ServiceAccount to use.\n    # If not set and create is true, a name is generated using the release name\n    name: ~\n\n    # Annotations to add to worker kubernetes service account.\n    annotations: {}\n\n  # A secret containing the connection\n  secretName: ~\n\n  # Else, if username and password are set, create secret from username and password\n  username: ~\n  password: ~\n\n  service:\n    type: ClusterIP\n    ## service annotations\n    annotations: {}\n    ports:\n      - name: flower-ui\n        port: \"{{ .Values.ports.flowerUI }}\"\n    # To change the port used to access flower:\n    # ports:\n    #   - name: flower-ui\n    #     port: 8080\n    #     targetPort: flower-ui\n    loadBalancerIP: ~\n    ## Limit load balancer source ips to list of CIDRs\n    # loadBalancerSourceRanges:\n    #   - \"10.123.0.0/16\"\n    loadBalancerSourceRanges: []\n\n  # Launch additional containers into the flower pods.\n  extraContainers: []\n  # Mount additional volumes into the flower pods. It can be templated like in the following example:\n  #   extraVolumes:\n  #     - name: my-templated-extra-volume\n  #       secret:\n  #          secretName: '{{ include \"my_secret_template\" . }}'\n  #          defaultMode: 0640\n  #          optional: true\n  #\n  #   extraVolumeMounts:\n  #     - name: my-templated-extra-volume\n  #       mountPath: \"{{ .Values.my_custom_path }}\"\n  #       readOnly: true\n  extraVolumes: []\n  extraVolumeMounts: []\n\n  # Select certain nodes for airflow flower pods.\n  nodeSelector: {}\n  affinity: {}\n  tolerations: []\n  topologySpreadConstraints: []\n\n  priorityClassName: ~\n\n  # annotations for the flower deployment\n  annotations: {}\n\n  podAnnotations: {}\n\n  # Labels specific to flower objects and pods\n  labels: {}\n  env: []\n\n# StatsD settings\nstatsd:\n  # Add custom annotations to the statsd configmap\n  configMapAnnotations: {}\n\n  enabled: true\n  # Max number of old replicasets to retain\n  revisionHistoryLimit: ~\n\n  # Arguments for StatsD exporter command.\n  args: [\"--statsd.mapping-config=/etc/statsd-exporter/mappings.yml\"]\n\n  # Annotations to add to the StatsD Deployment.\n  annotations: {}\n\n  # Create ServiceAccount\n  serviceAccount:\n    # default value is true\n    # ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/\n    automountServiceAccountToken: true\n    # Specifies whether a ServiceAccount should be created\n    create: true\n    # The name of the ServiceAccount to use.\n    # If not set and create is true, a name is generated using the release name\n    name: ~\n\n    # Annotations to add to worker kubernetes service account.\n    annotations: {}\n\n  uid: 65534\n  # When not set, `statsd.uid` will be used\n\n  # (deprecated, use `securityContexts` instead)\n  securityContext: {}\n  #  runAsUser: 65534\n  #  fsGroup: 0\n  #  runAsGroup: 0\n\n  # Detailed default security context for statsd deployments for container and pod level\n  securityContexts:\n    pod: {}\n    container: {}\n\n  # container level lifecycle hooks\n  containerLifecycleHooks: {}\n\n  # Additional network policies as needed\n  extraNetworkPolicies: []\n  resources: {}\n  #   limits:\n  #     cpu: 100m\n  #     memory: 128Mi\n  #   requests:\n  #     cpu: 100m\n  #     memory: 128Mi\n\n  service:\n    extraAnnotations: {}\n\n  # Select certain nodes for StatsD pods.\n  nodeSelector: {}\n  affinity: {}\n  tolerations: []\n  topologySpreadConstraints: []\n\n  priorityClassName: ~\n\n  # Additional mappings for StatsD exporter.\n  # If set, will merge default mapping and extra mappings, default mapping has higher priority.\n  # So, if you want to change some default mapping, please use `overrideMappings`\n  extraMappings: []\n\n  # Override mappings for StatsD exporter.\n  # If set, will ignore setting item in default and `extraMappings`.\n  # So, If you use it, ensure all mapping item contains in it.\n  overrideMappings: []\n\n  podAnnotations: {}\n  env: []\n\n# PgBouncer settings\npgbouncer:\n  # Enable PgBouncer\n  enabled: false\n  # Number of PgBouncer replicas to run in Deployment\n  replicas: 1\n  # Max number of old replicasets to retain\n  revisionHistoryLimit: ~\n  # Command to use for PgBouncer(templated).\n  command: [\"pgbouncer\", \"-u\", \"nobody\", \"/etc/pgbouncer/pgbouncer.ini\"]\n  # Args to use for PgBouncer(templated).\n  args: ~\n  auth_type: scram-sha-256\n  auth_file: /etc/pgbouncer/users.txt\n\n  # annotations to be added to the PgBouncer deployment\n  annotations: {}\n\n  podAnnotations: {}\n\n  # Create ServiceAccount\n  serviceAccount:\n    # default value is true\n    # ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/\n    automountServiceAccountToken: true\n    # Specifies whether a ServiceAccount should be created\n    create: true\n    # The name of the ServiceAccount to use.\n    # If not set and create is true, a name is generated using the release name\n    name: ~\n\n    # Annotations to add to worker kubernetes service account.\n    annotations: {}\n\n  # Additional network policies as needed\n  extraNetworkPolicies: []\n\n  # Pool sizes\n  metadataPoolSize: 10\n  resultBackendPoolSize: 5\n\n  # Maximum clients that can connect to PgBouncer (higher = more file descriptors)\n  maxClientConn: 100\n\n  # supply the name of existing secret with pgbouncer.ini and users.txt defined\n  # you can load them to a k8s secret like the one below\n  #  apiVersion: v1\n  #  kind: Secret\n  #  metadata:\n  #    name: pgbouncer-config-secret\n  #  data:\n  #     pgbouncer.ini: \u003cbase64_encoded pgbouncer.ini file content\u003e\n  #     users.txt: \u003cbase64_encoded users.txt file content\u003e\n  #  type: Opaque\n  #\n  #  configSecretName: pgbouncer-config-secret\n  #\n  configSecretName: ~\n\n  # PgBouncer pod disruption budget\n  podDisruptionBudget:\n    enabled: false\n\n    # PDB configuration\n    config:\n      # minAvailable and maxUnavailable are mutually exclusive\n      maxUnavailable: 1\n      # minAvailable: 1\n\n  # Limit the resources to PgBouncer.\n  # When you specify the resource request the k8s scheduler uses this information to decide which node to\n  # place the Pod on. When you specify a resource limit for a Container, the kubelet enforces those limits so\n  # that the running container is not allowed to use more of that resource than the limit you set.\n  # See: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/\n  # Example:\n  #\n  # resource:\n  #   limits:\n  #     cpu: 100m\n  #     memory: 128Mi\n  #   requests:\n  #     cpu: 100m\n  #     memory: 128Mi\n  resources: {}\n\n  service:\n    extraAnnotations: {}\n\n  # https://www.pgbouncer.org/config.html\n  verbose: 0\n  logDisconnections: 0\n  logConnections: 0\n\n  sslmode: \"prefer\"\n  ciphers: \"normal\"\n\n  ssl:\n    ca: ~\n    cert: ~\n    key: ~\n\n  # Add extra PgBouncer ini configuration in the databases section:\n  # https://www.pgbouncer.org/config.html#section-databases\n  extraIniMetadata: ~\n  extraIniResultBackend: ~\n  # Add extra general PgBouncer ini configuration: https://www.pgbouncer.org/config.html\n  extraIni: ~\n\n  # Mount additional volumes into pgbouncer. It can be templated like in the following example:\n  #   extraVolumes:\n  #     - name: my-templated-extra-volume\n  #       secret:\n  #          secretName: '{{ include \"my_secret_template\" . }}'\n  #          defaultMode: 0640\n  #          optional: true\n  #\n  #   extraVolumeMounts:\n  #     - name: my-templated-extra-volume\n  #       mountPath: \"{{ .Values.my_custom_path }}\"\n  #       readOnly: true\n  extraVolumes: []\n  extraVolumeMounts: []\n\n  # Launch additional containers into pgbouncer.\n  extraContainers: []\n\n  # Select certain nodes for PgBouncer pods.\n  nodeSelector: {}\n  affinity: {}\n  tolerations: []\n  topologySpreadConstraints: []\n\n  priorityClassName: ~\n\n  uid: 65534\n\n  # Detailed default security context for pgbouncer for container level\n  securityContexts:\n    pod: {}\n    container: {}\n\n  # container level lifecycle hooks\n  containerLifecycleHooks:\n    preStop:\n      exec:\n        # Allow existing queries clients to complete within 120 seconds\n        command: [\"/bin/sh\", \"-c\", \"killall -INT pgbouncer \u0026\u0026 sleep 120\"]\n\n  metricsExporterSidecar:\n    resources: {}\n    #  limits:\n    #   cpu: 100m\n    #   memory: 128Mi\n    #  requests:\n    #   cpu: 100m\n    #   memory: 128Mi\n    sslmode: \"disable\"\n\n    # supply the name of existing secret with PGBouncer connection URI containing\n    # stats user and password.\n    # you can load them to a k8s secret like the one below\n    #  apiVersion: v1\n    #  kind: Secret\n    #  metadata:\n    #    name: pgbouncer-stats-secret\n    #  data:\n    #     connection: postgresql://\u003cstats user\u003e:\u003cpassword\u003e@127.0.0.1:6543/pgbouncer?\u003cconnection params\u003e\n    #  type: Opaque\n    #\n    #  statsSecretName: pgbouncer-stats-secret\n    #\n    statsSecretName: ~\n\n    # Key containing the PGBouncer connection URI, defaults to `connection` if not defined\n    statsSecretKey: ~\n\n    # Detailed default security context for metricsExporterSidecar for container level\n    securityContexts:\n      container: {}\n\n    # container level lifecycle hooks\n    containerLifecycleHooks: {}\n\n    livenessProbe:\n      initialDelaySeconds: 10\n      periodSeconds: 10\n      timeoutSeconds: 1\n\n    readinessProbe:\n      initialDelaySeconds: 10\n      periodSeconds: 10\n      timeoutSeconds: 1\n\n  # Environment variables to add to pgbouncer container\n  env: []\n\n# Configuration for the redis provisioned by the chart\nredis:\n  enabled: true\n  terminationGracePeriodSeconds: 600\n\n  # Annotations for Redis Statefulset\n  annotations: {}\n\n  # Create ServiceAccount\n  serviceAccount:\n    # default value is true\n    # ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/\n    automountServiceAccountToken: true\n    # Specifies whether a ServiceAccount should be created\n    create: true\n    # The name of the ServiceAccount to use.\n    # If not set and create is true, a name is generated using the release name\n    name: ~\n\n    # Annotations to add to worker kubernetes service account.\n    annotations: {}\n\n  persistence:\n    # Enable persistent volumes\n    enabled: true\n    # Volume size for worker StatefulSet\n    size: 1Gi\n    # If using a custom storageClass, pass name ref to all statefulSets here\n    storageClassName:\n    # Annotations to add to redis volumes\n    annotations: {}\n\n  # Configuration for empty dir volume (if redis.persistence.enabled == false)\n  # emptyDirConfig:\n  #   sizeLimit: 1Gi\n  #   medium: Memory\n\n  resources: {}\n  #  limits:\n  #   cpu: 100m\n  #   memory: 128Mi\n  #  requests:\n  #   cpu: 100m\n  #   memory: 128Mi\n\n  # If set use as redis secret. Make sure to also set data.brokerUrlSecretName value.\n  passwordSecretName: ~\n\n  # Else, if password is set, create secret with it,\n  # Otherwise a new password will be generated on install\n  # Note: password can only be set during install, not upgrade.\n  password: ~\n\n  # This setting tells kubernetes that its ok to evict\n  # when it wants to scale a node down.\n  safeToEvict: true\n\n  # Select certain nodes for redis pods.\n  nodeSelector: {}\n  affinity: {}\n  tolerations: []\n  topologySpreadConstraints: []\n  priorityClassName: ~\n\n  # Set to 0 for backwards-compatiblity\n  uid: 0\n  # If not set, `redis.uid` will be used\n  securityContext: {}\n  #  runAsUser: 999\n  #  runAsGroup: 0\n\n  # Detailed default security context for redis for container and pod level\n  securityContexts:\n    pod: {}\n    container: {}\n\n  # container level lifecycle hooks\n  containerLifecycleHooks: {}\n\n  podAnnotations: {}\n# Auth secret for a private registry\n# This is used if pulling airflow images from a private registry\nregistry:\n  secretName: ~\n\n  # Example:\n  # connection:\n  #   user: ~\n  #   pass: ~\n  #   host: ~\n  #   email: ~\n  connection: {}\n\n# Elasticsearch logging configuration\nelasticsearch:\n  # Enable elasticsearch task logging\n  enabled: false\n  # A secret containing the connection\n  secretName: ~\n  # Or an object representing the connection\n  # Example:\n  # connection:\n  #   scheme: ~\n  #   user: ~\n  #   pass: ~\n  #   host: ~\n  #   port: ~\n  connection: {}\n\n# All ports used by chart\nports:\n  flowerUI: 5555\n  airflowUI: 8080\n  workerLogs: 8793\n  triggererLogs: 8794\n  redisDB: 6379\n  statsdIngest: 9125\n  statsdScrape: 9102\n  pgbouncer: 6543\n  pgbouncerScrape: 9127\n  # rpcServer support is experimental / dev purpose only and will later be renamed\n  _rpcServer: 9080\n\n# Define any ResourceQuotas for namespace\nquotas: {}\n\n# Define default/max/min values for pods and containers in namespace\nlimits: []\n\n# This runs as a CronJob to cleanup old pods.\ncleanup:\n  enabled: false\n  # Run every 15 minutes (templated).\n  schedule: \"*/15 * * * *\"\n  # To select a random-ish, deterministic starting minute between 3 and 12 inclusive for each release:\n  #     '{{- add 3 (regexFind \".$\" (adler32sum .Release.Name)) -}}-59/15 * * * *'\n  # To select the last digit of unix epoch time as the starting minute on each deploy:\n  #     '{{- now | unixEpoch | trunc -1 -}}-59/* * * * *'\n\n  # Command to use when running the cleanup cronjob (templated).\n  command: ~\n  # Args to use when running the cleanup cronjob (templated).\n  args: [\"bash\", \"-c\", \"exec airflow kubernetes cleanup-pods --namespace={{ .Release.Namespace }}\"]\n\n  # jobAnnotations are annotations on the cleanup CronJob\n  jobAnnotations: {}\n\n  # Select certain nodes for airflow cleanup pods.\n  nodeSelector: {}\n  affinity: {}\n  tolerations: []\n  topologySpreadConstraints: []\n  priorityClassName: ~\n\n  podAnnotations: {}\n\n  # Labels specific to cleanup objects and pods\n  labels: {}\n\n  resources: {}\n  #  limits:\n  #   cpu: 100m\n  #   memory: 128Mi\n  #  requests:\n  #   cpu: 100m\n  #   memory: 128Mi\n\n  # Create ServiceAccount\n  serviceAccount:\n    # default value is true\n    # ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/\n    automountServiceAccountToken: true\n    # Specifies whether a ServiceAccount should be created\n    create: true\n    # The name of the ServiceAccount to use.\n    # If not set and create is true, a name is generated using the release name\n    name: ~\n\n    # Annotations to add to cleanup cronjob kubernetes service account.\n    annotations: {}\n\n  # When not set, the values defined in the global securityContext will be used\n  securityContext: {}\n  #  runAsUser: 50000\n  #  runAsGroup: 0\n  env: []\n\n  # Detailed default security context for cleanup for container level\n  securityContexts:\n    pod: {}\n    container: {}\n\n  # container level lifecycle hooks\n  containerLifecycleHooks: {}\n\n  # Specify history limit\n  # When set, overwrite the default k8s number of successful and failed CronJob executions that are saved.\n  failedJobsHistoryLimit: ~\n  successfulJobsHistoryLimit: ~\n\n# Configuration for postgresql subchart\n# Not recommended for production\npostgresql:\n  enabled: true\n  auth:\n    enablePostgresUser: true\n    postgresPassword: postgres\n    username: \"\"\n    password: \"\"\n\n# Config settings to go into the mounted airflow.cfg\n#\n# Please note that these values are passed through the `tpl` function, so are\n# all subject to being rendered as go templates. If you need to include a\n# literal `{{` in a value, it must be expressed like this:\n#\n#    a: '{{ \"{{ not a template }}\" }}'\n#\n# Do not set config containing secrets via plain text values, use Env Var or k8s secret object\n# yamllint disable rule:line-length\nconfig:\n  core:\n    dags_folder: '{{ include \"airflow_dags\" . }}'\n    # This is ignored when used with the official Docker image\n    load_examples: 'False'\n    executor: '{{ .Values.executor }}'\n    # For Airflow 1.10, backward compatibility; moved to [logging] in 2.0\n    colored_console_log: 'False'\n    remote_logging: '{{- ternary \"True\" \"False\" .Values.elasticsearch.enabled }}'\n  logging:\n    remote_logging: '{{- ternary \"True\" \"False\" .Values.elasticsearch.enabled }}'\n    colored_console_log: 'False'\n  metrics:\n    statsd_on: '{{ ternary \"True\" \"False\" .Values.statsd.enabled }}'\n    statsd_port: 9125\n    statsd_prefix: airflow\n    statsd_host: '{{ printf \"%s-statsd\" (include \"airflow.fullname\" .) }}'\n  webserver:\n    enable_proxy_fix: 'True'\n    # For Airflow 1.10\n    rbac: 'True'\n  celery:\n    flower_url_prefix: '{{ ternary \"\" .Values.ingress.flower.path (eq .Values.ingress.flower.path \"/\") }}'\n    worker_concurrency: 16\n  scheduler:\n    standalone_dag_processor: '{{ ternary \"True\" \"False\" .Values.dagProcessor.enabled }}'\n    # statsd params included for Airflow 1.10 backward compatibility; moved to [metrics] in 2.0\n    statsd_on: '{{ ternary \"True\" \"False\" .Values.statsd.enabled }}'\n    statsd_port: 9125\n    statsd_prefix: airflow\n    statsd_host: '{{ printf \"%s-statsd\" (include \"airflow.fullname\" .) }}'\n    # `run_duration` included for Airflow 1.10 backward compatibility; removed in 2.0.\n    run_duration: 41460\n  elasticsearch:\n    json_format: 'True'\n    log_id_template: \"{dag_id}_{task_id}_{execution_date}_{try_number}\"\n  elasticsearch_configs:\n    max_retries: 3\n    timeout: 30\n    retry_timeout: 'True'\n  kerberos:\n    keytab: '{{ .Values.kerberos.keytabPath }}'\n    reinit_frequency: '{{ .Values.kerberos.reinitFrequency }}'\n    principal: '{{ .Values.kerberos.principal }}'\n    ccache: '{{ .Values.kerberos.ccacheMountPath }}/{{ .Values.kerberos.ccacheFileName }}'\n  celery_kubernetes_executor:\n    kubernetes_queue: 'kubernetes'\n  # The `kubernetes` section is deprecated in Airflow \u003e= 2.5.0 due to an airflow.cfg schema change.\n  # The `kubernetes` section can be removed once the helm chart no longer supports Airflow \u003c 2.5.0.\n  kubernetes:\n    namespace: '{{ .Release.Namespace }}'\n    # The following `airflow_` entries are for Airflow 1, and can be removed when it is no longer supported.\n    airflow_configmap: '{{ include \"airflow_config\" . }}'\n    airflow_local_settings_configmap: '{{ include \"airflow_config\" . }}'\n    pod_template_file: '{{ include \"airflow_pod_template_file\" . }}/pod_template_file.yaml'\n    worker_container_repository: '{{ .Values.images.airflow.repository | default .Values.defaultAirflowRepository }}'\n    worker_container_tag: '{{ .Values.images.airflow.tag | default .Values.defaultAirflowTag }}'\n    multi_namespace_mode: '{{ ternary \"True\" \"False\" .Values.multiNamespaceMode }}'\n  # The `kubernetes_executor` section duplicates the `kubernetes` section in Airflow \u003e= 2.5.0 due to an airflow.cfg schema change.\n  kubernetes_executor:\n    namespace: '{{ .Release.Namespace }}'\n    pod_template_file: '{{ include \"airflow_pod_template_file\" . }}/pod_template_file.yaml'\n    worker_container_repository: '{{ .Values.images.airflow.repository | default .Values.defaultAirflowRepository }}'\n    worker_container_tag: '{{ .Values.images.airflow.tag | default .Values.defaultAirflowTag }}'\n    multi_namespace_mode: '{{ ternary \"True\" \"False\" .Values.multiNamespaceMode }}'\n  triggerer:\n    default_capacity: 1000\n# yamllint enable rule:line-length\n\n# Whether Airflow can launch workers and/or pods in multiple namespaces\n# If true, it creates ClusterRole/ClusterRolebinding (with access to entire cluster)\nmultiNamespaceMode: false\n\n# `podTemplate` is a templated string containing the contents of `pod_template_file.yaml` used for\n# KubernetesExecutor workers. The default `podTemplate` will use normal `workers` configuration parameters\n# (e.g. `workers.resources`). As such, you normally won't need to override this directly, however,\n# you can still provide a completely custom `pod_template_file.yaml` if desired.\n# If not set, a default one is created using `files/pod-template-file.kubernetes-helm-yaml`.\npodTemplate: ~\n# The following example is NOT functional, but meant to be illustrative of how you can provide a custom\n# `pod_template_file`. You're better off starting with the default in\n# `files/pod-template-file.kubernetes-helm-yaml` and modifying from there.\n# We will set `priorityClassName` in this example:\n# podTemplate: |\n#   apiVersion: v1\n#   kind: Pod\n#   metadata:\n#     name: placeholder-name\n#     labels:\n#       tier: airflow\n#       component: worker\n#       release: {{ .Release.Name }}\n#   spec:\n#     priorityClassName: high-priority\n#     containers:\n#       - name: base\n#         ...\n\n# Git sync\ndags:\n  # Where dags volume will be mounted. Works for both persistence and gitSync.\n  # If not specified, dags mount path will be set to $AIRFLOW_HOME/dags\n  mountPath: ~\n  persistence:\n    # Annotations for dags PVC\n    annotations: {}\n    # Enable persistent volume for storing dags\n    enabled: false\n    # Volume size for dags\n    size: 1Gi\n    # If using a custom storageClass, pass name here\n    storageClassName:\n    # access mode of the persistent volume\n    accessMode: ReadWriteOnce\n    ## the name of an existing PVC to use\n    existingClaim:\n    ## optional subpath for dag volume mount\n    subPath: ~\n  gitSync:\n    enabled: true\n\n    # git repo clone url\n    # ssh example: git@github.com:apache/airflow.git\n    # https example: https://github.com/apache/airflow.git\n    repo: https://github.com/rafaelsmf/contratos-inteligentes-eth.git\n    branch: develop\n    rev: HEAD\n    # The git revision (branch, tag, or hash) to check out, v4 only\n    ref: develop\n    depth: 1\n    # the number of consecutive failures allowed before aborting\n    maxFailures: 0\n    # subpath within the repo where dags are located\n    # should be \"\" if dags are at repo root\n    subPath: \"applications/airflow/dags\"\n    # if your repo needs a user name password\n    # you can load them to a k8s secret like the one below\n    #   ---\n    #   apiVersion: v1\n    #   kind: Secret\n    #   metadata:\n    #     name: git-credentials\n    #   data:\n    #     # For git-sync v3\n    #     GIT_SYNC_USERNAME: \u003cbase64_encoded_git_username\u003e\n    #     GIT_SYNC_PASSWORD: \u003cbase64_encoded_git_password\u003e\n    #     # For git-sync v4\n    #     GITSYNC_USERNAME: \u003cbase64_encoded_git_username\u003e\n    #     GITSYNC_PASSWORD: \u003cbase64_encoded_git_password\u003e\n    # and specify the name of the secret below\n    #\n    credentialsSecret: git-credentials\n    #\n    #\n    # If you are using an ssh clone url, you can load\n    # the ssh private key to a k8s secret like the one below\n    #   ---\n    #   apiVersion: v1\n    #   kind: Secret\n    #   metadata:\n    #     name: airflow-ssh-secret\n    #   data:\n    #     # key needs to be gitSshKey\n    #     gitSshKey: \u003cbase64_encoded_data\u003e\n    # and specify the name of the secret below\n    # sshKeySecret: airflow-ssh-secret\n    #\n    # Or set sshKeySecret with your key\n    # sshKey: |-\n    #   -----BEGIN {OPENSSH PRIVATE KEY}-----\n    #   ...\n    #   -----END {OPENSSH PRIVATE KEY}-----\n    #\n    # If you are using an ssh private key, you can additionally\n    # specify the content of your known_hosts file, example:\n    #\n    # knownHosts: |\n    #    \u003chost1\u003e,\u003cip1\u003e \u003ckey1\u003e\n    #    \u003chost2\u003e,\u003cip2\u003e \u003ckey2\u003e\n\n    # interval between git sync attempts in seconds\n    # high values are more likely to cause DAGs to become out of sync between different components\n    # low values cause more traffic to the remote git repository\n    # Go-style duration string (e.g. \"100ms\" or \"0.1s\" = 100ms).\n    # For backwards compatibility, wait will be used if it is specified.\n    period: 5s\n    wait: ~\n    # add variables from secret into gitSync containers, such proxy-config\n    envFrom: ~\n    # envFrom: |\n    #   - secretRef:\n    #       name: 'proxy-config'\n\n    containerName: git-sync\n    uid: 65533\n\n    # When not set, the values defined in the global securityContext will be used\n    securityContext: {}\n    #  runAsUser: 65533\n    #  runAsGroup: 0\n\n    securityContexts:\n      container: {}\n\n    # container level lifecycle hooks\n    containerLifecycleHooks: {}\n\n    # Mount additional volumes into git-sync. It can be templated like in the following example:\n    #   extraVolumeMounts:\n    #     - name: my-templated-extra-volume\n    #       mountPath: \"{{ .Values.my_custom_path }}\"\n    #       readOnly: true\n    extraVolumeMounts: []\n    env: []\n    # Supported env vars for gitsync can be found at https://github.com/kubernetes/git-sync\n    # - name: \"\"\n    #   value: \"\"\n\n    # Configuration for empty dir volume\n    # emptyDirConfig:\n    #   sizeLimit: 1Gi\n    #   medium: Memory\n\n    resources: {}\n    #  limits:\n    #   cpu: 100m\n    #   memory: 128Mi\n    #  requests:\n    #   cpu: 100m\n    #   memory: 128Mi\n\nlogs:\n  # Configuration for empty dir volume (if logs.persistence.enabled == false)\n  # emptyDirConfig:\n  #   sizeLimit: 1Gi\n  #   medium: Memory\n\n  persistence:\n    # Enable persistent volume for storing logs\n    enabled: false\n    # Volume size for logs\n    size: 1Gi\n    # Annotations for the logs PVC\n    annotations: {}\n    # If using a custom storageClass, pass name here\n    storageClassName:\n    ## the name of an existing PVC to use\n    existingClaim:\n\n"
            ],
            "verify": false,
            "version": "1.15.0",
            "wait": false,
            "wait_for_jobs": false
          },
          "sensitive_attributes": [
            [
              {
                "type": "get_attr",
                "value": "repository_password"
              }
            ]
          ],
          "private": "eyJzY2hlbWFfdmVyc2lvbiI6IjEifQ==",
          "dependencies": [
            "kubectl_manifest.airflow_namespace",
            "kubectl_manifest.airflow_secrets"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "helm_release",
      "name": "postgres",
      "provider": "provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "atomic": false,
            "chart": "../../applications/postgres/postgres-1.4.0.tgz",
            "cleanup_on_fail": false,
            "create_namespace": false,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "postgres",
            "keyring": null,
            "lint": false,
            "manifest": null,
            "max_history": 0,
            "metadata": [
              {
                "app_version": "16.4-alpine3.20",
                "chart": "postgres",
                "first_deployed": 1729276229,
                "last_deployed": 1729276229,
                "name": "postgres",
                "namespace": "postgres",
                "notes": "1. Get the application URL by running these commands:\n  http://master-postgres.local/\n  http://read-replica-postgres.local/\n",
                "revision": 1,
                "values": "{\"affinity\":{\"podAntiAffinity\":{\"preferredDuringSchedulingIgnoredDuringExecution\":[{\"podAffinityTerm\":{\"labelSelector\":{\"matchExpressions\":[{\"key\":\"app.kubernetes.io/name\",\"operator\":\"In\",\"values\":[\"postgres\"]}]},\"topologyKey\":\"kubernetes.io/hostname\"},\"weight\":100}]}},\"autoscaling\":{\"enabled\":true,\"maxReplicas\":4,\"minReplicas\":2,\"targetCPUUtilizationPercentage\":80},\"configMap\":{},\"env\":[{\"name\":\"POSTGRES_USER\",\"value\":\"admin\"},{\"name\":\"POSTGRES_DB\",\"value\":\"crypto_ethereum\"},{\"name\":\"POSTGRES_PASSWORD\",\"valueFrom\":{\"secretKeyRef\":{\"key\":\"password\",\"name\":\"db-credentials\"}}}],\"fullnameOverride\":\"\",\"image\":{\"pullPolicy\":\"Always\",\"repository\":\"postgres\",\"tag\":\"16.4-alpine3.20\"},\"imagePullSecrets\":[],\"ingress\":{\"annotations\":{},\"className\":\"nginx\",\"enabled\":true,\"hosts\":[{\"host\":\"master-postgres.local\",\"paths\":[{\"backend\":{\"service\":{\"name\":\"postgres-master\",\"port\":{\"number\":5432}}},\"path\":\"/\",\"pathType\":\"Prefix\"}]},{\"host\":\"read-replica-postgres.local\",\"paths\":[{\"backend\":{\"service\":{\"name\":\"postgres-read-replica\",\"port\":{\"number\":5432}}},\"path\":\"/\",\"pathType\":\"Prefix\"}]}],\"tls\":[]},\"livenessProbe\":{\"enabled\":false,\"failureThreshold\":3,\"initialDelaySeconds\":10,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":5},\"nameOverride\":\"\",\"nodeSelector\":{},\"podAnnotations\":{},\"podLabels\":{\"app\":\"postgres\"},\"podSecurityContext\":{},\"readinessProbe\":{\"enabled\":false,\"failureThreshold\":3,\"initialDelaySeconds\":5,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":5},\"replicaCount\":2,\"resources\":{\"limits\":{\"cpu\":\"500m\",\"memory\":\"1Gi\"},\"requests\":{\"cpu\":\"500m\",\"memory\":\"1Gi\"}},\"securityContext\":{},\"service\":{\"port\":5432},\"serviceAccount\":{\"annotations\":{},\"automount\":false,\"create\":false,\"name\":\"\"},\"storageClass\":{\"name\":\"fast\",\"parameters\":{\"type\":\"pd-ssd\"},\"provisioner\":\"k8s.io/minikube-hostpath\"},\"tolerations\":[],\"volumeClaimTemplates\":{\"accessModes\":[\"ReadWriteOnce\"],\"name\":\"postgres-data\",\"resources\":{\"requests\":{\"storage\":\"1Gi\"}},\"storageClassName\":\"fast\"},\"volumeMounts\":[{\"mountPath\":\"/var/lib/postgresql/data\",\"name\":\"postgres-data\",\"readOnly\":false}],\"volumes\":{}}",
                "version": "1.4.0"
              }
            ],
            "name": "postgres",
            "namespace": "postgres",
            "pass_credentials": false,
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": null,
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [],
            "set_list": [],
            "set_sensitive": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 300,
            "upgrade_install": null,
            "values": [
              "# Default values for postgres.\n# This is a YAML-formatted file.\n# Declare variables to be passed into your templates.\n\n# This will set to have 1 master and 1 read replica for the postgres instance \nreplicaCount: 2\n\n# This sets the container image more information can be found here: https://kubernetes.io/docs/concepts/containers/images/\nimage:\n  repository: postgres\n  # This sets the pull policy for images.\n  pullPolicy: Always\n  # Overrides the image tag whose default is the chart appVersion.\n  tag: \"16.4-alpine3.20\"\n\n# Override environment variables\nenv:\n  - name: POSTGRES_USER\n    value: admin\n  - name: POSTGRES_DB\n    value: crypto_ethereum\n  - name: POSTGRES_PASSWORD\n    valueFrom:\n      secretKeyRef:\n        name: db-credentials\n        key: password  \n  \n# This is for the secretes for pulling an image from a private repository more information can be found here: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/\nimagePullSecrets: []\n# This is to override the chart name.\nnameOverride: \"\"\nfullnameOverride: \"\"\n\n#This section builds out the service account more information can be found here: https://kubernetes.io/docs/concepts/security/service-accounts/\nserviceAccount:\n  # Specifies whether a service account should be created\n  create: false\n  # Automatically mount a ServiceAccount's API credentials?\n  automount: false\n  # Annotations to add to the service account\n  annotations: {}\n  # The name of the service account to use.\n  # If not set and create is true, a name is generated using the fullname template\n  name: \"\"\n\n# This is for setting Kubernetes Annotations to a Pod.\n# For more information checkout: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/ \npodAnnotations: {}\n# This is for setting Kubernetes Labels to a Pod.\n# For more information checkout: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/\npodLabels:\n  app: postgres\n\npodSecurityContext: {}\n  # fsGroup: 1000\n\nsecurityContext: {}\n  # readOnlyRootFilesystem: false\n  # runAsUser: 1000\n  # runAsGroup: 1000\n  # fsGroup: 1000\n\n# This block is for setting up the ingress for more information can be found here: https://kubernetes.io/docs/concepts/services-networking/ingress/\ningress:\n  enabled: true\n  className: \"nginx\"\n  annotations: {}\n  hosts:\n    - host: master-postgres.local\n      paths:\n        - path: /\n          pathType: Prefix\n          backend:\n            service:\n              name: postgres-master\n              port:\n                number: 5432\n    - host: read-replica-postgres.local\n      paths:\n        - path: /\n          pathType: Prefix\n          backend:\n            service:\n              name: postgres-read-replica\n              port:\n                number: 5432\n  tls: []\n  #  - secretName: chart-example-tls\n  #    hosts:\n  #      - chart-example.local\n\nservice:\n  port: 5432\n\nresources:\n  limits:\n    cpu: 500m\n    memory: 1Gi\n  requests:\n    cpu: 500m\n    memory: 1Gi\n\n# This is to setup the liveness and readiness probes more information can be found here: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/\nlivenessProbe:\n  enabled: false\n  initialDelaySeconds: 10\n  periodSeconds: 10\n  timeoutSeconds: 5\n  successThreshold: 1\n  failureThreshold: 3\n\nreadinessProbe:\n  enabled: false\n  initialDelaySeconds: 5\n  periodSeconds: 10\n  timeoutSeconds: 5\n  successThreshold: 1\n  failureThreshold: 3\n\n#This section is for setting up autoscaling more information can be found here: https://kubernetes.io/docs/concepts/workloads/autoscaling/\nautoscaling:\n  enabled: true\n  minReplicas: 2\n  maxReplicas: 4\n  targetCPUUtilizationPercentage: 80\n  # targetMemoryUtilizationPercentage: 80\n\n# Additional volumeMounts on the output Deployment definition.\nvolumeMounts:\n- name: postgres-data\n  mountPath: \"/var/lib/postgresql/data\"\n  readOnly: false\n# - name: sql-files\n#   mountPath: \"/etc/sql-files\"\n#   readOnly: false\n\nvolumes: {}\n# - name: sql-files\n#   configMap:\n#     name: sql-files\n\nconfigMap: {}\n#   name: sql-files\n\n# Configuring a PVC\nvolumeClaimTemplates:\n  name: postgres-data\n  accessModes: [\"ReadWriteOnce\"]\n  storageClassName: fast\n  resources:\n    requests:\n      storage: 1Gi\n\n# Setting up the storage class\nstorageClass:\n  name: fast\n  provisioner: k8s.io/minikube-hostpath\n  parameters:\n    type: pd-ssd\n\nnodeSelector: {}\n\ntolerations: []\n\naffinity: \n  podAntiAffinity:\n    preferredDuringSchedulingIgnoredDuringExecution:\n    - weight: 100\n      podAffinityTerm:\n        labelSelector:\n          matchExpressions:\n          - key: app.kubernetes.io/name\n            operator: In\n            values:\n            - postgres\n        topologyKey: kubernetes.io/hostname\n"
            ],
            "verify": false,
            "version": "1.4.0",
            "wait": false,
            "wait_for_jobs": false
          },
          "sensitive_attributes": [
            [
              {
                "type": "get_attr",
                "value": "repository_password"
              }
            ]
          ],
          "private": "eyJzY2hlbWFfdmVyc2lvbiI6IjEifQ==",
          "dependencies": [
            "kubectl_manifest.postgres_namespace"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "helm_release",
      "name": "pyspark_notebook",
      "provider": "provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "atomic": false,
            "chart": "pyspark-notebook",
            "cleanup_on_fail": false,
            "create_namespace": false,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "pyspark-notebook",
            "keyring": null,
            "lint": false,
            "manifest": null,
            "max_history": 0,
            "metadata": [
              {
                "app_version": "1.16.0",
                "chart": "pyspark-notebook",
                "first_deployed": 1729258549,
                "last_deployed": 1729259151,
                "name": "pyspark-notebook",
                "namespace": "pyspark-notebook",
                "notes": "1. Get the application URL by running these commands:\n  export POD_NAME=$(kubectl get pods --namespace pyspark-notebook -l \"app.kubernetes.io/name=pyspark-notebook,app.kubernetes.io/instance=pyspark-notebook\" -o jsonpath=\"{.items[0].metadata.name}\")\n  export CONTAINER_PORT=$(kubectl get pod --namespace pyspark-notebook $POD_NAME -o jsonpath=\"{.spec.containers[0].ports[0].containerPort}\")\n  echo \"Visit http://127.0.0.1:8080 to use your application\"\n  kubectl --namespace pyspark-notebook port-forward $POD_NAME 8080:$CONTAINER_PORT\n",
                "revision": 2,
                "values": "{\"affinity\":{},\"blockManagerPort\":7777,\"disableToken\":true,\"driverPort\":2222,\"env\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"fullnameOverride\":\"\",\"httpPort\":8888,\"image\":{\"pullPolicy\":\"IfNotPresent\",\"repository\":\"jupyter/pyspark-notebook\",\"tag\":\"spark-3.5.0\"},\"imagePullSecrets\":[],\"ingress\":{\"annotations\":{},\"className\":\"\",\"enabled\":false,\"hosts\":[{\"host\":\"chart-example.local\",\"paths\":[{\"path\":\"/\",\"pathType\":\"ImplementationSpecific\"}]}],\"tls\":[]},\"nameOverride\":\"\",\"nodeSelector\":{},\"persistence\":{\"annotations\":{},\"enabled\":true,\"labels\":{\"enabled\":false}},\"podAnnotations\":{},\"podManagementPolicy\":\"Parallel\",\"podSecurityContext\":{\"fsGroup\":100,\"runAsUser\":1000},\"replicaCount\":1,\"resources\":{\"limits\":{\"cpu\":\"1000m\",\"memory\":\"1Gi\"},\"requests\":{\"cpu\":\"1000m\",\"memory\":\"1Gi\"}},\"secret\":[],\"securityContext\":{\"capabilities\":{\"drop\":[\"ALL\"]},\"runAsNonRoot\":true,\"runAsUser\":1000},\"service\":{\"annotations\":{},\"blockManagerPortName\":\"blockmanager\",\"driverPortName\":\"driver\",\"externalTrafficPolicy\":\"\",\"httpPortName\":\"http\",\"loadBalancerIP\":\"\",\"loadBalancerSourceRanges\":[],\"nodePort\":\"\",\"type\":\"ClusterIP\"},\"serviceAccount\":{\"annotations\":{},\"create\":true,\"name\":\"\"},\"terminationGracePeriod\":30,\"tolerations\":[],\"updateStrategy\":\"RollingUpdate\",\"volumeClaimTemplate\":{\"accessModes\":[\"ReadWriteOnce\"],\"resources\":{\"requests\":{\"storage\":\"1Gi\"}},\"storageClassName\":null,\"volumeMode\":\"Filesystem\"}}",
                "version": "0.2.2"
              }
            ],
            "name": "pyspark-notebook",
            "namespace": "pyspark-notebook",
            "pass_credentials": false,
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": "https://a3data.github.io/pyspark-notebook-helm/",
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [],
            "set_list": [],
            "set_sensitive": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 300,
            "upgrade_install": null,
            "values": [
              "# Default values for pyspark.\n# This is a YAML-formatted file.\n# Declare variables to be passed into your templates.\n\nreplicaCount: 1\n\nimage:\n  repository: jupyter/pyspark-notebook\n  pullPolicy: IfNotPresent\n  tag: \"spark-3.5.0\"\n\nimagePullSecrets: []\nnameOverride: \"\"\nfullnameOverride: \"\"\n\nvolumeClaimTemplate:\n  accessModes: [\"ReadWriteOnce\"]\n  volumeMode: Filesystem\n  resources:\n    requests:\n      storage: 1Gi\n  storageClassName:\n\npersistence:\n  enabled: true\n  labels:\n    # Add default labels for the volumeClaimTemplate fo the StatefulSet\n    enabled: false\n  annotations: {}\n\n# Extra environment variables to append to pyspark container\n# This will be appended to the current 'env:' key. You can use any of the kubernetes env\n# syntax here\nenv: []\n  # - name: GOOGLE_APPLICATION_CREDENTIALS\n  #   value: /mnt/secrets/key.json\n\n# Allows you to load environment variables from kubernetes secret\nsecret: []\n# - envName: AWS_ACCESS_KEY_ID\n#   secretName: \"\"\n#   secretKey: \"\"\n# - envName: AWS_SECRET_ACCESS_KEY\n#   secretName: \"\"\n#   secretKey: \"\"\n\n# Mount additional volumes into pyspark container.\nextraVolumes: []\n#   - name: secrets\n#     secret:\n#       secretName: gcp-credentials\n\nextraVolumeMounts: []\n#   - name: secrets\n#     mountPath: \"/mnt/secrets\"\n#     readOnly: true\n\nserviceAccount:\n  # Specifies whether a service account should be created\n  create: true\n  # Annotations to add to the service account\n  annotations: {}\n  # The name of the service account to use.\n  # If not set and create is true, a name is generated using the fullname template\n  name: \"\"\n\npodAnnotations: {}\n\ndisableToken: true\n\npodSecurityContext:\n  fsGroup: 100\n  runAsUser: 1000\n\nsecurityContext:\n  capabilities:\n    drop:\n    - ALL\n  # readOnlyRootFilesystem: true\n  runAsNonRoot: true\n  runAsUser: 1000\n\n# The default is to deploy all pods serially. By setting this to parallel all pods are started at\n# the same time when bootstrapping the cluster\npodManagementPolicy: \"Parallel\"\n\nhttpPort: 8888\nblockManagerPort: 7777\ndriverPort: 2222\n\nservice:\n  type: ClusterIP\n  nodePort: \"\"\n  annotations: {}\n  httpPortName: http\n  blockManagerPortName: blockmanager\n  driverPortName: driver\n  loadBalancerIP: \"\"\n  loadBalancerSourceRanges: []\n  externalTrafficPolicy: \"\"\n\nupdateStrategy: RollingUpdate\n\n# How long to wait for pyspark to stop gracefully\nterminationGracePeriod: 30\n\ningress:\n  enabled: false\n  className: \"\"\n  annotations: {}\n    # kubernetes.io/ingress.class: nginx\n    # kubernetes.io/tls-acme: \"true\"\n  hosts:\n    - host: chart-example.local\n      paths:\n        - path: /\n          pathType: ImplementationSpecific\n  tls: []\n  #  - secretName: chart-example-tls\n  #    hosts:\n  #      - chart-example.local\n\nresources:\n  requests:\n    cpu: \"1000m\"\n    memory: \"1Gi\"\n  limits:\n    cpu: \"1000m\"\n    memory: \"1Gi\"\n\nnodeSelector: {}\n\ntolerations: []\n\naffinity: {}"
            ],
            "verify": false,
            "version": "0.2.2",
            "wait": false,
            "wait_for_jobs": false
          },
          "sensitive_attributes": [
            [
              {
                "type": "get_attr",
                "value": "repository_password"
              }
            ]
          ],
          "private": "eyJzY2hlbWFfdmVyc2lvbiI6IjEifQ==",
          "dependencies": [
            "kubectl_manifest.pyspark_notebook_namespace"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "helm_release",
      "name": "spark",
      "provider": "provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "atomic": false,
            "chart": "spark-operator",
            "cleanup_on_fail": false,
            "create_namespace": false,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "spark",
            "keyring": null,
            "lint": false,
            "manifest": null,
            "max_history": 0,
            "metadata": [
              {
                "app_version": "2.0.2",
                "chart": "spark-operator",
                "first_deployed": 1729258556,
                "last_deployed": 1729258556,
                "name": "spark",
                "namespace": "spark-operator",
                "notes": "",
                "revision": 1,
                "values": "{}",
                "version": "2.0.2"
              }
            ],
            "name": "spark",
            "namespace": "spark-operator",
            "pass_credentials": false,
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": "https://kubeflow.github.io/spark-operator",
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [],
            "set_list": [],
            "set_sensitive": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 300,
            "upgrade_install": null,
            "values": null,
            "verify": false,
            "version": "2.0.2",
            "wait": false,
            "wait_for_jobs": false
          },
          "sensitive_attributes": [
            [
              {
                "type": "get_attr",
                "value": "repository_password"
              }
            ]
          ],
          "private": "eyJzY2hlbWFfdmVyc2lvbiI6IjEifQ==",
          "dependencies": [
            "kubectl_manifest.gcp_secrets",
            "kubectl_manifest.spark_operator_clusterrolebinding",
            "kubectl_manifest.spark_operator_namespace",
            "kubectl_manifest.spark_operator_serviceaccount"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "kubectl_manifest",
      "name": "airflow_namespace",
      "provider": "provider[\"registry.terraform.io/gavinbunney/kubectl\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "api_version": "v1",
            "apply_only": false,
            "force_conflicts": false,
            "force_new": false,
            "id": "/api/v1/namespaces/airflow",
            "ignore_fields": null,
            "kind": "Namespace",
            "live_manifest_incluster": "3873eb09f6ead575ac4b7c686e819f1161a90b6fc06216ff32e6f35179525fbe",
            "live_uid": "cf4412fb-b1bd-4308-beea-72c621ef641b",
            "name": "airflow",
            "namespace": null,
            "override_namespace": null,
            "sensitive_fields": null,
            "server_side_apply": false,
            "timeouts": null,
            "uid": "cf4412fb-b1bd-4308-beea-72c621ef641b",
            "validate_schema": true,
            "wait": null,
            "wait_for_rollout": true,
            "yaml_body": "apiVersion: v1\nkind: Namespace\nmetadata:\n  name: airflow",
            "yaml_body_parsed": "apiVersion: v1\nkind: Namespace\nmetadata:\n  name: airflow\n",
            "yaml_incluster": "3873eb09f6ead575ac4b7c686e819f1161a90b6fc06216ff32e6f35179525fbe"
          },
          "sensitive_attributes": [
            [
              {
                "type": "get_attr",
                "value": "live_manifest_incluster"
              }
            ],
            [
              {
                "type": "get_attr",
                "value": "yaml_incluster"
              }
            ],
            [
              {
                "type": "get_attr",
                "value": "yaml_body"
              }
            ]
          ],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDB9LCJzY2hlbWFfdmVyc2lvbiI6IjEifQ=="
        }
      ]
    },
    {
      "mode": "managed",
      "type": "kubectl_manifest",
      "name": "airflow_secrets",
      "provider": "provider[\"registry.terraform.io/gavinbunney/kubectl\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "api_version": "v1",
            "apply_only": false,
            "force_conflicts": false,
            "force_new": false,
            "id": "/api/v1/namespaces/airflow/secrets/git-credentials",
            "ignore_fields": null,
            "kind": "Secret",
            "live_manifest_incluster": "b26c3eb079f4a40fc63e741279aa32fba652a4b657bd52d89caa71d69e1d7eb7",
            "live_uid": "a8002e4f-e6db-414b-ab28-690263cc12c3",
            "name": "git-credentials",
            "namespace": "airflow",
            "override_namespace": null,
            "sensitive_fields": null,
            "server_side_apply": false,
            "timeouts": null,
            "uid": "a8002e4f-e6db-414b-ab28-690263cc12c3",
            "validate_schema": true,
            "wait": null,
            "wait_for_rollout": true,
            "yaml_body": "apiVersion: v1\nkind: Secret\nmetadata:\n  name: git-credentials\n  namespace: airflow\ndata:\n  # For git-sync v4\n  GITSYNC_USERNAME: Z2l0LXN5bmMK\n  GITSYNC_PASSWORD: Z2l0aHViX3BhdF8xMUFKU0xaNkkwYk1pMllwTkh2Z1E4X2dJNkF4OVU1ckpPZXNWSDlMaENEalN3VEVIME5wSzRiQXlqdHh0RlU4YXFRNVNaSk1GSUYwVmxZRHNpCg==\n  # For git-sync v3\n  GIT_SYNC_USERNAME: Z2l0LXN5bmMK\n  GIT_SYNC_PASSWORD: Z2l0aHViX3BhdF8xMUFKU0xaNkkwYk1pMllwTkh2Z1E4X2dJNkF4OVU1ckpPZXNWSDlMaENEalN3VEVIME5wSzRiQXlqdHh0RlU4YXFRNVNaSk1GSUYwVmxZRHNpCg==",
            "yaml_body_parsed": "apiVersion: v1\ndata: (sensitive value)\nkind: Secret\nmetadata:\n  name: git-credentials\n  namespace: airflow\n",
            "yaml_incluster": "b26c3eb079f4a40fc63e741279aa32fba652a4b657bd52d89caa71d69e1d7eb7"
          },
          "sensitive_attributes": [
            [
              {
                "type": "get_attr",
                "value": "yaml_body"
              }
            ],
            [
              {
                "type": "get_attr",
                "value": "live_manifest_incluster"
              }
            ],
            [
              {
                "type": "get_attr",
                "value": "yaml_incluster"
              }
            ]
          ],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDB9LCJzY2hlbWFfdmVyc2lvbiI6IjEifQ=="
        }
      ]
    },
    {
      "mode": "managed",
      "type": "kubectl_manifest",
      "name": "postgres_namespace",
      "provider": "provider[\"registry.terraform.io/gavinbunney/kubectl\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "api_version": "v1",
            "apply_only": false,
            "force_conflicts": false,
            "force_new": false,
            "id": "/api/v1/namespaces/postgres",
            "ignore_fields": null,
            "kind": "Namespace",
            "live_manifest_incluster": "7880bb6db80ecd09bc73483642068f2aecbb8c4960711d2fad7ebb2cb3a31765",
            "live_uid": "68d71008-f1c5-4268-9aa9-93337916d55e",
            "name": "postgres",
            "namespace": null,
            "override_namespace": null,
            "sensitive_fields": null,
            "server_side_apply": false,
            "timeouts": null,
            "uid": "68d71008-f1c5-4268-9aa9-93337916d55e",
            "validate_schema": true,
            "wait": null,
            "wait_for_rollout": true,
            "yaml_body": "apiVersion: v1\nkind: Namespace\nmetadata:\n  name: postgres",
            "yaml_body_parsed": "apiVersion: v1\nkind: Namespace\nmetadata:\n  name: postgres\n",
            "yaml_incluster": "7880bb6db80ecd09bc73483642068f2aecbb8c4960711d2fad7ebb2cb3a31765"
          },
          "sensitive_attributes": [
            [
              {
                "type": "get_attr",
                "value": "yaml_incluster"
              }
            ],
            [
              {
                "type": "get_attr",
                "value": "yaml_body"
              }
            ],
            [
              {
                "type": "get_attr",
                "value": "live_manifest_incluster"
              }
            ]
          ],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDB9LCJzY2hlbWFfdmVyc2lvbiI6IjEifQ=="
        }
      ]
    },
    {
      "mode": "managed",
      "type": "kubectl_manifest",
      "name": "pyspark_notebook_namespace",
      "provider": "provider[\"registry.terraform.io/gavinbunney/kubectl\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "api_version": "v1",
            "apply_only": false,
            "force_conflicts": false,
            "force_new": false,
            "id": "/api/v1/namespaces/pyspark-notebook",
            "ignore_fields": null,
            "kind": "Namespace",
            "live_manifest_incluster": "32718e3cb606f09eb011631d3ab8049514b95b882d0250663619d05539132681",
            "live_uid": "f1cce354-80bb-4917-9063-c719feeb7047",
            "name": "pyspark-notebook",
            "namespace": null,
            "override_namespace": null,
            "sensitive_fields": null,
            "server_side_apply": false,
            "timeouts": null,
            "uid": "f1cce354-80bb-4917-9063-c719feeb7047",
            "validate_schema": true,
            "wait": null,
            "wait_for_rollout": true,
            "yaml_body": "apiVersion: v1\nkind: Namespace\nmetadata:\n  name: pyspark-notebook",
            "yaml_body_parsed": "apiVersion: v1\nkind: Namespace\nmetadata:\n  name: pyspark-notebook\n",
            "yaml_incluster": "32718e3cb606f09eb011631d3ab8049514b95b882d0250663619d05539132681"
          },
          "sensitive_attributes": [
            [
              {
                "type": "get_attr",
                "value": "live_manifest_incluster"
              }
            ],
            [
              {
                "type": "get_attr",
                "value": "yaml_incluster"
              }
            ],
            [
              {
                "type": "get_attr",
                "value": "yaml_body"
              }
            ]
          ],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDB9LCJzY2hlbWFfdmVyc2lvbiI6IjEifQ=="
        }
      ]
    },
    {
      "mode": "managed",
      "type": "kubectl_manifest",
      "name": "spark_operator_clusterrolebinding",
      "provider": "provider[\"registry.terraform.io/gavinbunney/kubectl\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "api_version": "rbac.authorization.k8s.io/v1",
            "apply_only": false,
            "force_conflicts": false,
            "force_new": false,
            "id": "/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/spark-role",
            "ignore_fields": null,
            "kind": "ClusterRoleBinding",
            "live_manifest_incluster": "13f31934f422fb3062736d3e0293a067b1804b497be659ab94aecc39283d0157",
            "live_uid": "a333bf44-05e2-43ae-9873-aa9bc6107dc5",
            "name": "spark-role",
            "namespace": null,
            "override_namespace": null,
            "sensitive_fields": null,
            "server_side_apply": false,
            "timeouts": null,
            "uid": "a333bf44-05e2-43ae-9873-aa9bc6107dc5",
            "validate_schema": true,
            "wait": null,
            "wait_for_rollout": true,
            "yaml_body": "apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: spark-role\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: edit\nsubjects:\n  - kind: ServiceAccount\n    name: spark\n    namespace: spark-operator",
            "yaml_body_parsed": "apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: spark-role\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: edit\nsubjects:\n- kind: ServiceAccount\n  name: spark\n  namespace: spark-operator\n",
            "yaml_incluster": "13f31934f422fb3062736d3e0293a067b1804b497be659ab94aecc39283d0157"
          },
          "sensitive_attributes": [
            [
              {
                "type": "get_attr",
                "value": "yaml_incluster"
              }
            ],
            [
              {
                "type": "get_attr",
                "value": "live_manifest_incluster"
              }
            ],
            [
              {
                "type": "get_attr",
                "value": "yaml_body"
              }
            ]
          ],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDB9LCJzY2hlbWFfdmVyc2lvbiI6IjEifQ=="
        }
      ]
    },
    {
      "mode": "managed",
      "type": "kubectl_manifest",
      "name": "spark_operator_namespace",
      "provider": "provider[\"registry.terraform.io/gavinbunney/kubectl\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "api_version": "v1",
            "apply_only": false,
            "force_conflicts": false,
            "force_new": false,
            "id": "/api/v1/namespaces/spark-operator",
            "ignore_fields": null,
            "kind": "Namespace",
            "live_manifest_incluster": "ec5907ab48f22c3d340b87d97d24ad08d40fd9806b26ec2381414739d223a81c",
            "live_uid": "f214aa7f-5701-413f-8fa5-2ca500742e4d",
            "name": "spark-operator",
            "namespace": null,
            "override_namespace": null,
            "sensitive_fields": null,
            "server_side_apply": false,
            "timeouts": null,
            "uid": "f214aa7f-5701-413f-8fa5-2ca500742e4d",
            "validate_schema": true,
            "wait": null,
            "wait_for_rollout": true,
            "yaml_body": "apiVersion: v1\nkind: Namespace\nmetadata:\n  name: spark-operator",
            "yaml_body_parsed": "apiVersion: v1\nkind: Namespace\nmetadata:\n  name: spark-operator\n",
            "yaml_incluster": "ec5907ab48f22c3d340b87d97d24ad08d40fd9806b26ec2381414739d223a81c"
          },
          "sensitive_attributes": [
            [
              {
                "type": "get_attr",
                "value": "yaml_incluster"
              }
            ],
            [
              {
                "type": "get_attr",
                "value": "live_manifest_incluster"
              }
            ],
            [
              {
                "type": "get_attr",
                "value": "yaml_body"
              }
            ]
          ],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDB9LCJzY2hlbWFfdmVyc2lvbiI6IjEifQ=="
        }
      ]
    },
    {
      "mode": "managed",
      "type": "kubectl_manifest",
      "name": "spark_operator_serviceaccount",
      "provider": "provider[\"registry.terraform.io/gavinbunney/kubectl\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "api_version": "v1",
            "apply_only": false,
            "force_conflicts": false,
            "force_new": false,
            "id": "/api/v1/namespaces/spark-operator/serviceaccounts/spark",
            "ignore_fields": null,
            "kind": "ServiceAccount",
            "live_manifest_incluster": "7948548239da6e146e4a74f3de59a859a7863d0409f63351e9a7ab5f780b4d88",
            "live_uid": "d3e9bd2c-3693-4b6f-8ee7-da9610df8c68",
            "name": "spark",
            "namespace": "spark-operator",
            "override_namespace": null,
            "sensitive_fields": null,
            "server_side_apply": false,
            "timeouts": null,
            "uid": "d3e9bd2c-3693-4b6f-8ee7-da9610df8c68",
            "validate_schema": true,
            "wait": null,
            "wait_for_rollout": true,
            "yaml_body": "apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: spark\n  namespace: spark-operator",
            "yaml_body_parsed": "apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: spark\n  namespace: spark-operator\n",
            "yaml_incluster": "7948548239da6e146e4a74f3de59a859a7863d0409f63351e9a7ab5f780b4d88"
          },
          "sensitive_attributes": [
            [
              {
                "type": "get_attr",
                "value": "yaml_body"
              }
            ],
            [
              {
                "type": "get_attr",
                "value": "yaml_incluster"
              }
            ],
            [
              {
                "type": "get_attr",
                "value": "live_manifest_incluster"
              }
            ]
          ],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDB9LCJzY2hlbWFfdmVyc2lvbiI6IjEifQ=="
        }
      ]
    }
  ],
  "check_results": null
}
