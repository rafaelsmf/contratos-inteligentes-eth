apiVersion: sparkoperator.k8s.io/v1beta2
kind: SparkApplication
metadata:
  name: bigquery-to-postgres-job
  namespace: processing
spec:
  type: Python
  pythonVersion: "3"
  mode: cluster
  image: spark:3.5.3
  imagePullPolicy: IfNotPresent
  mainApplicationFile: local:///scripts/develop/scripts/contratos-inteligentes-eth/jobs/bigquery_to_postgres.py
  arguments: 
    # - "{{ ds }}"
    - "2024-10-20"
  timeToLiveSeconds: 60
  sparkVersion: "3.5.3"
  restartPolicy:
    type: Never
  hadoopConf:
    fs.gs.impl: com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem
    google.cloud.auth.service.account.enable: "true"
    google.cloud.auth.service.account.json.keyfile: "/etc/secrets/gcp-credentials.json"
  driver:
    cores: 1
    coreRequest: '1000m'
    coreLimit: "1000m"    
    memory: "2000m"
    serviceAccount: spark
    volumeMounts:
      - name: scripts
        mountPath: /scripts
      - name: git-secret
        mountPath: /etc/git-secret
        readOnly: true
    initContainers:
      - name: git-sync
        image: "k8s.gcr.io/git-sync/git-sync:v3.6.1"
        imagePullPolicy: Always
        volumeMounts:
        - name: scripts
          mountPath: /scripts
        - name: git-secret
          mountPath: /etc/git-secret
          readOnly: true
        env: 
        - name: GIT_SYNC_REPO
          value: "https://github.com/rafaelsmf/contratos-inteligentes-eth.git"
        - name: GIT_SYNC_BRANCH
          value: "develop"
        - name: GIT_SYNC_ROOT
          value: "/scripts"
        - name: GIT_SYNC_DEST
          value: "develop"
        - name: GIT_SYNC_ONE_TIME
          value: "true"
        - name: GIT_SYNC_VERBOSE
          value: "true"
        - name: GIT_SYNC_USERNAME
          value: Z2l0LXN5bmMK
        - name: GIT_SYNC_PASSWORD
          value: Z2l0aHViX3BhdF8xMUFKU0xaNkkwYk1pMllwTkh2Z1E4X2dJNkF4OVU1ckpPZXNWSDlMaENEalN3VEVIME5wSzRiQXlqdHh0RlU4YXFRNVNaSk1GSUYwVmxZRHNpCg==
  executor:
    cores: 1
    coreRequest: '1000m'
    coreLimit: "1000m"    
    instances: 1
    memory: "2000m"
    serviceAccount: spark
    securityContext:
      runAsUser: 1000
      runAsGroup: 1000
  volumes:
    - name: scripts
      emptyDir: {}
    - name: git-secret
      secret:
        secretName: gcp-credentials