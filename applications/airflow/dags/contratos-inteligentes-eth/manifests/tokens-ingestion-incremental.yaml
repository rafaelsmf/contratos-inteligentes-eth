apiVersion: "sparkoperator.k8s.io/v1beta2"
kind: SparkApplication
metadata:
  name: bigquery-to-postgres-job
  namespace: processing
spec:
  type: Python
  pythonVersion: "3"
  mode: cluster
  image: "gcr.io/spark-operator/spark-py:v3.1.1"
  mainApplicationFile: local:///scripts/contratos-inteligentes-eth/jobs/bigquery_to_postgres.py
  timeToLiveSeconds: 60
  sparkVersion: "3.1.1"
  restartPolicy:
    type: Never
  hadoopConf:
    fs.gs.impl: com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem
    google.cloud.auth.service.account.enable: "true"
    google.cloud.auth.service.account.json.keyfile: "/etc/secrets/gcp-credentials.json"
  driver:
    cores: 1
    coreRequest: '100m'
    coreLimit: "500m"    
    memory: "2000m"
    serviceAccount: spark
    # securityContext:
    #   fsGroup: 65534
    volumeMounts:
      - name: scripts
        mountPath: /scripts
      - name: git-secret
        mountPath: /etc/git-secret
        readOnly: true
    initContainers:
      - name: git-sync
        image: "k8s.gcr.io/git-sync/git-sync:v3.6.1"
        env:
          - name: GIT_SYNC_REPO
            value: "https://github.com/rafaelsmf/contratos-inteligentes-eth.git"
          - name: GIT_SYNC_BRANCH
            value: "develop"
          - name: GIT_SYNC_ROOT
            value: "/scripts"
          - name: GIT_SYNC_ONE_TIME
            value: "true"
  executor:
    cores: 1
    coreRequest: '100m'
    coreLimit: "500m"    
    instances: 2
    memory: "2000m"
    serviceAccount: spark
    # securityContext:
    #   fsGroup: 65534
  volumes:
    - name: scripts
      emptyDir: {}
    - name: git-secret
      secret:
        secretName: gcp-credentials