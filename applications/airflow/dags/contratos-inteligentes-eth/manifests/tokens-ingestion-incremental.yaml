apiVersion: sparkoperator.k8s.io/v1beta2
kind: SparkApplication
metadata:
  name: bigquery-to-postgres-job
  namespace: processing
spec:
  type: Python
  pythonVersion: "3"
  mode: cluster
  image: rafaelsousamf/spark-with-bigquery-connector:3.5.3-scala2.12
  imagePullPolicy: IfNotPresent
  mainApplicationFile: local:///scripts/develop/scripts/contratos-inteligentes-eth/jobs/bigquery_to_postgres.py
  arguments: 
    # - "{{ ds }}"
    - "2024-10-20"
  timeToLiveSeconds: 60
  sparkVersion: "3.5.3"
  restartPolicy:
    type: Never
  hadoopConf:
    fs.gs.project.id: "desafio-stone-439013"
    fs.gs.impl: com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem
    google.cloud.auth.service.account.enable: "true"
    google.cloud.auth.service.account.json.keyfile: "/etc/secrets/gcp-credentials.json"
  driver:
    cores: 1
    coreRequest: '1000m'
    coreLimit: "1000m"    
    memory: "2000m"
    serviceAccount: spark
    secrets:
      - name: "gcp-credentials"
        path: "/etc/secrets/gcp-credentials"
        secretType: GCPServiceAccount
    envVars:
      GCS_PROJECT_ID: "desafio-stone-439013"
    volumeMounts:
      - name: scripts
        mountPath: /scripts
    initContainers:
      - name: git-sync
        image: "k8s.gcr.io/git-sync/git-sync:v3.6.1"
        imagePullPolicy: Always
        volumeMounts:
        - name: scripts
          mountPath: /scripts
        env: 
        - name: GIT_SYNC_REPO
          value: "https://github.com/rafaelsmf/contratos-inteligentes-eth.git"
        - name: GIT_SYNC_BRANCH
          value: "develop"
        - name: GIT_SYNC_ROOT
          value: "/scripts"
        - name: GIT_SYNC_DEST
          value: "develop"
        - name: GIT_SYNC_ONE_TIME
          value: "true"
        - name: GIT_SYNC_VERBOSE
          value: "true"
        - name: GIT_SYNC_USERNAME
          value: Z2l0LXN5bmMK
        - name: GIT_SYNC_PASSWORD
          value: Z2l0aHViX3BhdF8xMUFKU0xaNkkwYk1pMllwTkh2Z1E4X2dJNkF4OVU1ckpPZXNWSDlMaENEalN3VEVIME5wSzRiQXlqdHh0RlU4YXFRNVNaSk1GSUYwVmxZRHNpCg==
  executor:
    cores: 1
    coreRequest: '1000m'
    coreLimit: "1000m"    
    instances: 1
    memory: "2000m"
    serviceAccount: spark
    securityContext:
      runAsUser: 1000
      runAsGroup: 1000
    secrets:
      - name: "gcp-credentials"
        path: "/etc/secrets/gcp-credentials"
        secretType: GCPServiceAccount
    envVars:
      GCS_PROJECT_ID: "desafio-stone-439013"
  volumes:
    - name: scripts
      emptyDir: {}