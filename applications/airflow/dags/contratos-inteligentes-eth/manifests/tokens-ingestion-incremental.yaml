apiVersion: "sparkoperator.k8s.io/v1beta2"
kind: SparkApplication
metadata:
  name: bigquery-to-postgres-job
  namespace: spark-operator
spec:
  type: Python
  pythonVersion: "3"
  mode: cluster
  image: "voidmarcus/spark-py:3.5.0"
  mainApplicationFile: local:///scripts/contratos-inteligentes-etch/jobs/bigquery_to_postgres.py
  arguments:
    - "{{ ds }}" 
  sparkVersion: "3.5.0"
  restartPolicy:
    type: Never
  driver:
    cores: 1
    coreLimit: "1000m"
    memory: "2g"
    labels:
      version: 3.5.0
    serviceAccount: spark
  executor:
    cores: 1
    instances: 2
    memory: "4g"
    labels:
      version: 3.5.0
  initContainers:
      - name: git-sync
        image: "k8s.gcr.io/git-sync/git-sync:v3.6.1"
        imagePullPolicy: Always
        env: 
        - name: GIT_SYNC_REPO
          value: "https://github.com/rafaelsmf/contratos-inteligentes-eth.git"
        - name: GIT_SYNC_BRANCH
          value: "develop"
        - name: GIT_SYNC_ROOT
          value: /scripts
        - name: GIT_SYNC_DEST
          value: "develop"
        - name: GIT_SYNC_ONE_TIME
          value: "true"
        - name: GIT_SYNC_USERNAME
          value: Z2l0LXN5bmMK
        - name: GIT_SYNC_PASSWORD
          value: Z2l0aHViX3BhdF8xMUFKU0xaNkkwYk1pMllwTkh2Z1E4X2dJNkF4OVU1ckpPZXNWSDlMaENEalN3VEVIME5wSzRiQXlqdHh0RlU4YXFRNVNaSk1GSUYwVmxZRHNpCg==
  hadoopConf:
    fs.gs.impl: com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem
    google.cloud.auth.service.account.enable: true
    google.cloud.auth.service.account.json.keyfile: "/etc/secrets/gcp-credentials.json"
  volumes:
    - name: gcp-credentials
      secret:
        secretName: gcp-credentials 
  volumeMounts:
    - name: gcp-credentials
      mountPath: /etc/secrets/gcp-credentials.json
      subPath: gcp-credentials.json